# Interpretability of Neural Networks

## 1. Introduction

> 可解释性是深度学习的重要一环，重要到非常影响DL最后能不能在大规模在实际应用中落地。我同学曾经举过一个例子，他们公司的一个项目需要在文本中做实体识别，之前一直是用正则表达式进行规则式的匹配。这种路子的好处是可修改性强，人工操作影响的效果十分直接，任务简单的时候效率也高，准确率也不差。但是缺点也显而易见，随着业务的扩大，这种暴力匹配很快就难以满足需求了。于是他们上了深度学习技术，其效果确实提升了不少。但是随着带来的问题是，深度学习模型产生了一些令人困惑的错误，比如会把地名完全错误地识别成人名，这样一来集成到产品中，就会给用户带来非常不好的体验。换言之，深度学习效果好是平均意义上的，具体到某些bad case，深度学习错的一塌糊涂，且是驴唇不对马嘴。
>
> 最要命的问题是，DL还不知道如何纠正这些错误。不同于规则式的手法，人工经验教给深度网络改正这些错误，而且产生立竿见影的效果，是比较困难的。唯一的办法是不断地用错误数据打上标签继续训练，这无疑是一项笨拙而又耗时的工作。
>
> 所以我认为，**深度学习可解释性，不只是解释深度学习如果工作，更应该引申一层，即如何有效控制神经网络的行为**。近年来，对抗样本研究的兴起，已经揭开了深度学习脆弱性的冰山一角。这便是很好的佐证。如果无法有效控制神经网络，无法有效理解网络中到底发生了什么，人们一定会对这种“黑盒”抱有足够的怀疑和担心。-- 知乎张旭



> 在端对端学习之外，我觉得还需要找到一套新的神经网络操作工具，即让神经网络具有清晰的符号化的内部知识表达，去匹配人类自身的知识框架，从而人们可以在语义层面对神经网络进行诊断和修改。从logic-based专家系统，到graphical model，再到深度神经网络，模型的flexibility和performance逐渐提高。但是，从相反的方向，把一个神经网络的内部逻辑转化成graphical representations，或者logic-based rules，从而提高知识表达的interpretability。有了清晰的内部表达，那么对神经网络的训练是不是不但可以end-to-end，而且可以end-to-middle，middle-to-middle？当网络内部一些单元具有了某种语义，那么transfer learning是不是直接在语义层面指派就好了，不需要大数据去训练了？当网络训练可以深入到网络的内部语义，或许deep learning未来的发展会有更多的可能性。
>
> 我希望一个CNN不仅仅告诉我它在某张图像上检测到一只小鸟，我还要CNN明确的告诉我，它用第一个filter去监测鸟头，第二个filter去检测鸟尾巴。因为这两个filter被这张图像触发，所以判断出图像中有一只小鸟。进一步，当我知道鸟的分类得分是0.7，我还希望CNN给出鸟头部分贡献了0.3的分数，鸟尾贡献了0.2。当CNN内部逻辑足够条理清晰，我们是否还需要通过大数据进行端对端的训练？我们能否在语义层面直接debug CNN呢？
>
> 沿着这条思路，在“Interpreting CNN knowledge via an Explanatory Graph”in AAAI 2018一文中，我主要介绍了如何把一个CNN（pre-trained for object classification）的conv-layer内部知识转化成一个graphical model。算法自动学习出一个explanatory graph with tens of thousands of nodes去解释CNN内部的hierarchical知识结构。Explanatory graph中每一个node，严格表示在CNN中某个conv-layer的某个object part pattern。这样我就可以把混乱的CNN的知识拆分成几十万个object parts的子patterns。
>
> 在另一篇文章"Interpretable Convolutional Neural Networks" in CVPR 2018 (spotlight)中，我介绍了如何端对端的学习一个CNN，使得其内部高层conv-layer的每个filter自动的表示某个object part。算法并不需要人为的标注object parts或texture作为额外的supervision，而是为这些filters添加一个prior constraint，使得在训练过程中自动回归的某种object part。-- 上海交大张拳石

### 1.1 深度学习的可解释性研究概述

*文章来源：知乎王小贱 -- 北航BIGSCity研究组*

#### 1.1.1 **可解释性定义：**

Interpretation is the process of giving explanations **to Human**.

#### 1.1.2 **为什么需要可解释性：**

- 我们希望知道模型究竟从数据中学到了哪些知识（以人类可以理解的方式表达的）从而产生了最终的决策。
- 不可解释同样也意味着**危险**，如对抗样本。具备可解释性的模型在面对这些问题的时候是可以对异常产生的原因进行追踪和定位的
- *可解释的必要性也存在争议*

#### 1.1.3 可解释性方法

- 在建模之前的可解释性方法
- 建立本身具备可解释性的模型
- 在建模之后使用可解释性方法对模型作出解释

#### 1.1.4 在建模之前的可解释性方法

这一类方法其实主要涉及一些数据预处理或数据展示的方法。机器学习解决的是从数据中发现知识和规律的问题，在建模之前的可解释性方法的关键在于帮助我们**迅速而全面地了解数据分布的特征**，从而帮助我们考虑在建模过程中可能面临的问题并选择一种最合理的模型来逼近问题所能达到的最优解。

**数据可视化：**

在真正要研究一个数据问题之前，通过建立一系列方方面面的可视化方法来**建立我们对数据的直观理解**是非常必须的，特别是当数据量非常大或者数据维度非常高的时候，比如一些时空高维数据，如果可以建立一些交互式的可视化方法将会极大地帮助我们**从各个层次角度理解数据的分布**。

**探索性质的数据分析：**

帮助我们更好地理解数据的分布情况。比如一种称为MMD-critic方法中，可以帮助我们找到数据中一些具有代表性或者不具代表性的样本。

#### 1.1.5 建立本身具备可解释性的模型

**1. 基于规则的方法（Rule-based）**

**2. 基于单个特征的方法（Per-feature-based）**

**3. 基于实例的方法（Case-based）**

**4. 稀疏性方法（Sparsity）**

**5. 单调性方法（Monotonicity）**

**基于规则的方法:**

比如决策树模型。这类模型中任何的一个决策都可以对应到一个逻辑规则表示。但当规则表示过多或者原始的特征本身就不是特别好解释的时候，基于规则的方法有时候也不太适用。

**基于单个特征的方法:**主要是一些非常经典的线性模型，比如线性回归、逻辑回归、广义线性回归、广义加性模型等，这类模型可以说是现在可解释性最高的方法

**基于实例的方法:**

主要是通过一些代表性的样本来解释聚类/分类结果的方法。比如下图所展示的贝叶斯实例模型（Bayesian Case Model，BCM），我们将样本分成三个组团，可以分别找出每个组团中具有的代表性样例和重要的子空间。比如对于下面第一类聚类来说，绿脸是具有代表性的样本，而绿色、方块是具有代表性的特征子空间。基于实例的方法的一些局限在于可能挑出来的样本不具有代表性或者人们可能会有过度泛化的倾向。

<img src=assets/bcm.jpg width=400>

**基于稀疏性的方法:**

主要是利用信息的稀疏性特质，将模型尽可能地简化表示。比如一种图稀疏性的LDA方法，根据层次性的单词信息形成了层次性的主题表达，这样一些小的主题就可以被更泛化的主题所概括，从而可以使我们更容易理解特定主题所代表的含义。

**基于单调性的方法**：

在很多机器学习问题中，有一些输入和输出之间存在正相关/负相关关系，如果在模型训练中我们可以找出这种单调性的关系就可以让模型具有更高的可解释性。比如医生对患特定疾病的概率的估计主要由一些跟该疾病相关联的高风险因素决定，找出单调性关系就可以帮助我们识别这些高风险因素*。*



*除了这些通用性的可解释性模型，也有一部分工作旨在建立本身具有可解释性的**深度学习模型**。*



#### **1.1.6 在建模之后使用可解释性性方法作出解释**

建模后的可解释性方法主要是针对具有黑箱性质的深度学习模型而言的，主要分为以下几类的工作：

**1. 隐层分析方法**

**2. 敏感性分析方法**

**3. 模拟/代理模型**



### 1.2 隐层分析方法

深度学习的黑箱性主要来源于其高度非线性性质，每个神经元都是由上一层的线性组合再加上一个非线性函数的得到，我们无法像理解线性回归的参数那样通过非常solid的统计学基础假设来理解神经网络中的参数含义及其重要程度、波动范围。但实际上我们是知道这些参数的具体值以及整个训练过程的，所以**神经网络模型本身其实并不是一个黑箱，其黑箱性在于我们没办法用人类可以理解的方式理解模型的具体含义和行为，**而神经网络的一个非常好的性质在于神经元的分层组合形式，这让我们可以用物质组成的视角来理解神经网络的运作方式。比如人体的组成过程是从分子-细胞-组织-器官-系统-人体。而通过一些对神经网络隐层的可视化我们也发现：比如一个人脸识别的例子，神经网络在这个过程中先学到了边角的概念，之后学到了五官，最后学到了整个面部的特征。

*分析出来每个隐藏层的功能，调参时可以有针对性的调整训练？*

#### 1.2.1 Visualizing and Understanding Convolutional Networks

要理解神经网络中每层都学到了哪些概念一个非常直观的方法就是通过对隐层运用一些可视化方法来将其转化成人类可以理解的有实际含义的图像，这方面代表性的一个工作就是14年ECCV这篇文章。本文主要利用了反卷积的相关思想实现了特征可视化来帮助我们理解CNN的每一层究竟学到了什么东西。典型的CNN模型的一个完整卷积过程是由卷积-激活-池化（pooling）三个步骤组成的。而如果想把一个CNN的中间层转化成原始输入空间呢？我们就需要经过反池化-反激活-反卷积这样的一个逆过程。

**反池化：**

max pooling过程中我们将3x3的一个pooling块中的最大值取出，而unpooling则是将pooling后的值恢复成3x3的像素单元，由于我们现在只有一个激活值， 所以只将**该激活值对应原pooling块中位置的值还原回去**，其他的值设定成0。所以在max-pooling的时候，我们不光要知道pooling值，同时也要记录下**pooling值的对应位置**，比如下图pooling值的位置就是(0,1)。

<img src=assets/upooling.jpg width=400>

**反激活：**

在典型的CNN模型中，我们一般使用Relu作为激活函数，而反激活的值和实际的激活值没有任何区别：只保留正数，其余值为0即可。

**反卷积：**

反卷积的真实含义应该是**转置卷积**（Transposed Convolution），CNN模型的卷积过程本质上来讲和一般的神经网络没有任何区别（只不过将一些共用参数组合成了一个滤波器的形式），都可以转变成一个**矩阵乘法的操作**（只不过对CNN模型来说是一个参数重复度很高的稀疏矩阵），我们不妨将这个稀疏矩阵表示为 ![[公式]](https://www.zhihu.com/equation?tex=C) ，那么后一层和前一层的关系就可以表示为：

![[公式]](https://www.zhihu.com/equation?tex=CX%5E%7Bl%7D+%3D+X%5E%7Bl%2B1%7D)

而在反向传播的过程中， ![[公式]](https://www.zhihu.com/equation?tex=C%5ET) 往往可以表示为**卷积层对输入层的梯度**，也就是说通过对卷积层进行适当的补0操作，再用原始的卷积核转置之后的卷积核进行卷积操作，就可以得到**相应的梯度矩阵与当前卷积层的乘积**。而我们在这里使用反池化-反激活之后的特征（其中包含了大部分为0的数值）进行该操作其实表征了**原始输入对池化之后的特征的影响**，因为在反激活过程中保证了所有值非负因此反卷积的过程中符号不会发生改变。

通过上面的介绍我们其实可以明白，这个反卷积的方法之所以能够成功地将CNN的隐层可视化出来，**一个关键就在于通过反激活-反池化的过程，我们屏蔽掉了很多对当前层的激活值没有实际作用的输入层的影响将其归为0，通过反卷积操作就得到了仅对当前层有实际贡献的输入层的数值——将其作为该层的特征表示**。**因为我们最后得到的这个特征表示和原输入空间的大小是一致的，其数值表示也对应着原始空间的像素点，所以在一定程度上，这是我们可以理解的一个特征表示。** 

反卷积是一个上采样过程：

<img src=assets/trans_conv.jpg width=300 >

- 从实验结果可以看出来，第二层对应着一些边角或色彩特征，第三层对应着纹理特征，第四层对应着一些如狗脸、车轮这样的局部部位，第五层则对整体的物体有较强的识别能力。

#### 1.2.2 Network Dissection: Quantifying Interpretability of Deep Visual Representations

通过上面这篇论文的工作，我们可以大致地用肉眼来判断神经网络学到的概念类型，但如果能识别一些语义概念的话对我们来说可能更有意义，在这方面一个非常有代表性的工作是在CVPR 2017上发表的这篇文章，提出了一种**网络切割(Network Dissection)**的方法来提取CNN的概念表示，分为三个步骤：

1. 识别一个范围很广的人工标注的视觉语义概念数据集

2. 将隐层变量对应到这些概念表示上

3. 量化这些隐层-概念对的匹配程度

首先， 为了获得大量的视觉语义概念数据，研究人员收集了来自不同数据源的分层语义标注数据（包括颜色、材质、材料、部分、物体、场景这几个层次）。

然后，通过如下方法将隐层变量对应到这些概念表示上：对于每个输入图像x，获取每个隐层k的activation map ![[公式]](https://www.zhihu.com/equation?tex=A_k%28x%29) （其实也就是feature map），这样就可以得到隐层k激活值的分布，对于每个隐层k，我们都可以找到一个 ![[公式]](https://www.zhihu.com/equation?tex=T_k) 使得 ![[公式]](https://www.zhihu.com/equation?tex=P%28A_k+%3E+T_k%29+%3D+0.005) ，这个 ![[公式]](https://www.zhihu.com/equation?tex=T_k) 可以作为接下来判断区域是否激活的一个标准。为了方便对比低分辨率的卷积层和输入层的概念激活热图 ![[公式]](https://www.zhihu.com/equation?tex=L_c%28x%29) （其实就是标注出了相关概念在图像中的代表区域），我们将低分辨率的卷积层的特征图 ![[公式]](https://www.zhihu.com/equation?tex=A_k%28x%29) 通过插值的方法扩展为和原始图片尺寸一样大的图像 ![[公式]](https://www.zhihu.com/equation?tex=S_k%28X%29) 。之后再建立一个二元分割 ![[公式]](https://www.zhihu.com/equation?tex=M_k%28x%29%3DS_k%28x%29%5Cge+T_k) ，这样就得到了所有被激活的区域。

最后，通过将 ![[公式]](https://www.zhihu.com/equation?tex=M_k%28x%29) 和输入层的概念激活热图 ![[公式]](https://www.zhihu.com/equation?tex=L_c%28x%29) 作对比，获得隐层-概念对的匹配程度：

![img](https://pic2.zhimg.com/80/v2-b333ca2f5c5fb61158af8dfb361d4b1d_720w.jpg)

可以发现**如果匹配度高的话，那么分子就比较大（交叉范围大），分母就比较小（合并范围小），我们通过和颜色、材质、材料、部分、物体、场景不同层次的概念作匹配就能得到隐层学到的概念层次了**。

- 从实验结果中我们也可以发现随着层数的增加，神经网络学到的概念类型也逐渐变得高级，比如在AlexNet中，前面的卷积层对颜色、材质的识别力较强，后面的卷积层对物体、场景的识别力较强。特别是对物体的识别上，后面的卷积层有突出的优势。

#### 1.2.3 How transferable are features in deep neural networks

从低级概念到高级概念的一个过程中总是会伴随着一个非常有意思的现象：泛化性逐渐降低，特化性逐渐升高。比如在细胞层次上，人类和其他动物的区别比较小，这个层次的泛化性就高，但到组织器官层次区别就比较大，这个层次的特化性就高。Bengio团队在2014年发表的这篇工作就是通过研究特征的可迁移性来对这个从泛化的特化的过程进行评估。

**特征在迁移任务上的表现往往是评价特征泛化性能的一个非常好的依据。**在迁移学习中，我们首先基于基础数据训练一个基础网络，然后将特征改换到另一个任务上，如果特征是具备泛化性的，那么其在迁移任务中应该也是适用的。

在这个工作中，作者将1000个ImageNet的分类分成了两个组，每个组个包含大约500个分类和645000个样本。然后利用这两组数据各训练一个八层的卷积网络baseA和baseB，然后分别取第1到第7个卷积层训练几个新的网络，以第3层为例：

- 自我复制网络（selffer network）B3B，前三层从baseB上复制并冻结。剩余的5个卷积层随机初始化并在数据集B上训练，这个网络作为控制组

- 一个迁移网络（transfer network）A3B：前三层从baseA上复制并冻结。剩余的5个卷积层随机初始化并在数据集B上训练。如果A3B的任务表现和baseB一样好，那么就说明第三层的特征至少对于B任务来讲是泛化的，如果不如baseB，那么说明第三层特征就是特化的。

- 一个自我复制网络B3B+，网络结构和B3B一样，但所有层都是可学习的

- 一个迁移网络A3B+，网络结构和A3B一样，但所有层都是可学习的

这些网络的结构图如下图所示：

<img src=assets/b3b.jpg width=400>

- 从实验结果来看，transferAnB的随着层数的增加性能在不断下降（泛化降低，特化提升，这印证了我们对泛化特化性质随层数变化的基本判断），而控制组的selfferBnB的性能出现了先下降后上升的现象（泛化和特化都不足够的中间层既缺乏可学习性，特征的描述性又不够强，因而出现了性能下降的现象），transferBnB+和transferAnB+一直维持着比较好的性能，但其中transferAnB+的性能确是最好的，特征在迁移任务上表现出来的优势其实也对应了我们在上一节中讲的**模型本身也意味着知识**。
- *感觉本文结果对我理解我的网络没有什么用。。。*

#### 1.2.4 Understanding intermediate layers using linear classifier probes

对于神经网络来说，隐层的数量永远都是一个玄学，我们如何理解隐层的数量和模型性能之间的关系呢？Bengio在2016年的这个工作思路非常简单，就是通过在每个隐层中添加一个线性探针来测试隐层的表征性能。什么是线性探针呢？也很简单，就是以每个隐藏层为输入，判别的label为输出建立一个逻辑回归模型，通过评估模型的准确率我们就能得到隐层在整个训练过程中以及训练结束之后表征性能的变化。

<img src=assets/probes.jpg width=400 >

- 通过32个隐层在二分数据上的实验我们可以发现随着隐层的增加表征性能不断提高，但提高的比率也逐渐趋于缓慢。
- 在基于Minist数据训练的CNN模型上，经过10个周期的训练，第一个卷积层的表征性能提升非常明显，但之后的卷积层并没有很明显的提升。
- *感觉只是这种方法也许可以用到个人对自己网络结构的探索上，也没啥通用性的结论。*





### 1.3 变量敏感性分析

隐层分析法的一个问题在于，通过端到端训练的隐层很多时候并没有什么特定的含义，多数依赖我们的主观判断（*感觉知道了隐藏层的含义也没啥用，对模型架构的设计、训练、调整有什么启发意义吗？*）。但是深度学习模型中往往只有输入层对我们来说才是有意义的，所以了解深度学习模型的一个更直观的方法是**通过研究输入层的变化对结果的影响来判断输入变量或输入样本的重要性**，这也是通常所说的敏感性分析方法。

敏感性分析（Sensitivity Analysis）是一类非常重要的，用于定量描述模型输入变量对输出变量的重要性程度的方法。假设模型表示为 ![[公式]](https://www.zhihu.com/equation?tex=y%3Df%28x_1%2Cx_2%2C...%2Cx_n%29) ，**敏感性分析就是令每个属性在可能的范围变动，研究和预测这些属性的变化对模型输出值的影响程度**。我们将影响程度的大小称为该属性的敏感性系数，敏感性系数越大，就说明属性对模型输出的影响越大。一般来讲对于神经网络的敏感性分析方法可以分为**变量敏感性分析、样本敏感性分析**两种，变量敏感性分析用来检验输入属性变量对模型的影响程度，样本敏感性分析用来研究具体样本对模型的重要程度，也是敏感性分析研究的一个新方向。

神经网络中的变量敏感性分析方法大概有以下几种：基于连接权的敏感性方法，基于偏导的敏感性分析方法、通过改变输入变量观察其影响的方法和与统计方法结合的敏感性分析方法。

#### 1.3.1 基于连接权的敏感性分析方法

基于连接权的方法中比较有代表性的工作是Garson等人1991年在**《Interpreting neural network connection weights》**提出的方法，这种来自于“远古时期”的智慧相对来说就要简单粗暴一点。输入变量 ![[公式]](https://www.zhihu.com/equation?tex=x_i+) 对输出变量 ![[公式]](https://www.zhihu.com/equation?tex=y_k) 的影响程度为：

![[公式]](https://www.zhihu.com/equation?tex=Q_%7Bik%7D+%3D+%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5EL%28w_%7Bij%7Dv_%7Bjk%7Dg%2F%5Csum_%7Br%3D1%7D%5ENw_%7Bij%7D%29%7D%7B%5Csum_%7Bi%3D1%7D%5EN%5Csum_%7Bj%3D1%7D%5EL%28w_%7Bij%7Dv_%7Bjk%7Dg%2F%5Csum_%7Br%3D1%7D%5ENw_%7Brj%7D%29%7D)

但是由于这种方法中 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bij%7D) 的正负性导致我们可能没办法得到真实的敏感性系数，所以后来就对这个公式进行了改造，改为用绝对值来评估影响力。

![[公式]](https://www.zhihu.com/equation?tex=Q_%7Bik%7D%27+%3D+%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5EL%28%7Cw_%7Bij%7Dv_%7Bjk%7D%7Cg%2F%5Csum_%7Br%3D1%7D%5EN%7Cw_%7Bij%7D%7C%29%7D%7B%5Csum_%7Bi%3D1%7D%5EN%5Csum_%7Bj%3D1%7D%5EL%28%7Cw_%7Bij%7Dv_%7Bjk%7D%7Cg%2F%5Csum_%7Br%3D1%7D%5EN%7Cw_%7Brj%7D%7C%29%7D)

<img src=assets/garson.jpg width=500 >

<img src=assets/garson_2.jpg width=500 >

当然这种方法放到一个两个隐层的网络可能还适用，放到深度网络中由于忽略了非线性激活函数误差会一步一步积累，所以慢慢也就被大家抛弃了。



#### **1.3.2 基于统计方法的敏感性分析方法**

基于统计方法的敏感性分析方法中一个比较有代表性的工作是Olden等人2002年在Elsevier上发表的**《Illuminating the “black box”: a randomization approach for understanding variable contributions in artificial neural networks》**，虽然也是来自于“远古时期”的智慧，却是我个人非常欣赏的一个工作，现在看来依然觉得非常牛逼。这个工作的步骤如下：

1. 使用随机初始的权重构建一组神经网络
2. 从中选出预测性能最好的神经网络，记录下该网络的初始权重，计算并记录：
   - 输入神经元对输出神经值经过某个隐层神经元的连接权重：也就是上面Garson算法图中的 ![[公式]](https://www.zhihu.com/equation?tex=c_%7BA1%7D+)
   - 全局的连接权重，也就是每个变量输入层到输出层连接权的总和，也就是上面Garson算法图中的 ![[公式]](https://www.zhihu.com/equation?tex=c_1+%3D+c_%7BA1%7D%2Bc_%7BB1%7D)
   - 根据前面所提到Garson算法计算每个变量的相对重要性

3. 随机打乱输出值( ![[公式]](https://www.zhihu.com/equation?tex=y_%7Brandom%7D) )

4. 使用 ![[公式]](https://www.zhihu.com/equation?tex=y_%7Brandom%7D) 和初始的随机权重构建一个神经网络

5. 大量重复步骤3和4，每次都记录步骤2中的值

这个方法牛逼的地方在于通过大量的重复采样（bootstrap）使得**对神经网络的统计检验**成为了可能，随机打乱了输出值使我们能够得到**基于给定初始值随机训练网络的权重和重要性分布**，这样就可以通过统计检验的方法来（如果pvalue非常小，那么我们可以认为这个值不是随机选的，是显著的）。



#### **1.3.3 基于偏导的敏感性分析方法**

基于偏导的敏感性分析方法主要是利用偏导数来评估输入变量对输出的影响，比如Dimoponlos等人在**《Use of some sensitivity criteria for choosing networks with good generalization ability》**中提出的敏感性方法，除了考虑偏导以外，作者也考虑了曲率（也就是二阶导数）的影响，因为神经网络在激活函数处的非线性导致了虽然导数绝对值较小，但有可能因为曲率较大导致敏感性高的情况，某个变量j的敏感性可以由所有样本在变量j处的导数和曲率乘积的平方和度量：



![img](https://pic4.zhimg.com/80/v2-e805102325f96cb086b587f5b08e0bc7_720w.jpg)

#### **1.3.4 基于输入变量扰动的敏感性分析方法**

基于输入变量扰动的敏感性分析方法是在前向网络中最为常用的一种敏感性分析方法，可以用于评估变量的重要性从而达到变量筛选的目的，比如Dombi等人提出的**平均影响值（MIV，Mean Impact Value）方法**就是一种被广泛应用的评估方法，MIV方法的计算过程如下：

1. 在网络训练终止后，将训练样本P中每一自变量特征在原值的基础上分别加/减10%后成两个训练样本P1和P2
2. 将P1和P2分别作为仿真样本利用已建成的网络进行仿真，得到两个仿真结果A1和A2
3. 求出A1和A2的差值，即为变动该自变量后对输出产生的影响变化值
4. 最后将IV按观测样本数平均得到该自变量对应变量——网络输出的MIV。

按照MIV的绝对值排序即可得到各个自变量对网络输出影响的重要性排序，从而**完成变量筛选的目的**。

*搞清楚了每个变量对输出的重要性，一定程度上对模型的工作原理进行了解释*

### **1.4 样本敏感性分析**

样本敏感性分析中比较有代表性的工作是 Pang Wei Koh 2017年在ICML上发表的**Understanding Black-box Predictions via Influence Functions**，这篇文章也是去年ICML的best paper，通过影响力函数来理解深度学习黑盒模型的预测效果。什么是影响力函数呢？这个其实就涉及到统计学的相关概念了，大家都知道一般来说我们训练神经网络模型也好还是别的SVM之类的什么模型都是通过SGD等方法训练的，找到一个目标损失函数然后让它不断收敛得到最终的参数。在理想状况下最后收敛的点应该都是导数为0的点，那么我们如何评估某个样本的敏感度呢？有个办法就是将样本做微小的改变 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) ，这样最后收敛的参数也会发生改变，然后得到参数的改变对 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) 的导数就是这个样本的影响力函数。

通过影响力函数的计算可以发现，对下图的测试样本7来说，右边的训练样本的存在对该样本的判定是有害的。

 <img src=assets/influ_fun.jpg width=300> 



由于计算出了对训练样本施加轻微扰动之后对特定测试样本损失函数的影响，所以这个方法也可以应用到对抗样本的生成中，只需要在一部分影响力函数较大的样本中添加一些肉眼不可见的扰动，就足以干扰其他样本的判定结果。



## 相关资源

**论文：**

A Survey Of Methods For Explaining Black Box Models

论文列表 https://zhuanlan.zhihu.com/p/74542589

**特征可视化工具：**

https://github.com/tensorflow/lucid

https://github.com/utkuozbulak/pytorch-cnn-visualizations
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained models with TF-Slim\n",
    "Like Keras, it also offers a nice variety of pretrained CNN models to download and use.\n",
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim/python/slim/nets/\n",
    "\n",
    "## TF-Slim\n",
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n",
    "\n",
    "TF-Slim is a relatively new lightweight extension of TensorFlow that, like other\n",
    "abstractions, allows us to define and train complex models quickly and\n",
    "intuitively. TF-Slim doesn’t require any installation since it’s been merged with Ten‐\n",
    "sorFlow.\n",
    "\n",
    "This extension is all about convolutional neural networks. CNNs are notorious for\n",
    "having a lot of messy boilerplate code. TF-Slim was designed with the goal of opti‐\n",
    "mizing the creation of very complex CNN models so that they could be elegantly\n",
    "written and easy to interpret and debug by using high-level layers, variable abstrac‐\n",
    "tions, and argument scoping, which we will touch upon shortly.\n",
    "\n",
    "In addition to enabling us to create and train our own models, TF-Slim has available\n",
    "pretrained networks that can be easily downloaded, read, and used: VGG, AlexNet,\n",
    "Inception, and more.\n",
    "\n",
    "We start this section by briefly describing some of TF-Slim’s abstraction features.\n",
    "Then we shift our focus to how to download and use a pretrained model, demonstrat‐\n",
    "ing it for the VGG image classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TF-Slim we can create a variable easily by defining its initialization, regulariza‐\n",
    "tion, and device with one wrapper. For example, here we define weights initialized\n",
    "from a truncated normal distribution using L2 regularization and placed on the CPU\n",
    "(we will talk about distributing model parts across devices in Chapter 9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "W = slim.variable('w', shape=[7,7,3, 3],\n",
    "                  initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                  regularizer=slim.l2_regularizer(0.07),\n",
    "                  device='/CPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-Slim can reduce a lot of\n",
    "boilerplate code and redundant duplication. As with Keras or TFLearn, we can define\n",
    "a layer operation at an abstract level to include the convolution operation, weights\n",
    "initialization, regularization, activation function, and more in a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = slim.conv2d(inputs, 64, [11,11], 4, padding='SAME',\n",
    "                 weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                 weights_regularizer=slim.l2_regularizer(0.0007), scope='conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-Slim extends its elegance even beyond that, providing a clean way to replicate lay‐\n",
    "ers compactly by using the repeat , stack , and arg_scope commands.\n",
    "\n",
    "repeat saves us the need to copy and paste the same line over and over so that, for\n",
    "example, instead of having this redundant duplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = slim.conv2d(net, 128, [3,3], scope='conv1_1')\n",
    "net = slim.conv2d(net, 128, [3,3], scope='conv1_2')\n",
    "net = slim.conv2d(net, 128, [3,3], scope='conv1_3')\n",
    "net = slim.conv2d(net, 128, [3,3], scope='conv1_4')\n",
    "net = slim.conv2d(net, 128, [3,3], scope='conv1_5')\n",
    "\n",
    "# equivalent:\n",
    "net = slim.repeat(net, 5, slim.conv2d, 128, [3,3], scope='con1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is viable only in cases where we have layers of the same size. When this does\n",
    "not hold, we can use the stack command, allowing us to concatenate layers of differ‐\n",
    "ent shapes. So, instead of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = slim.conv2d(net, 64, [3,3], scope='conv1_1')\n",
    "net = slim.conv2d(net, 64, [1,1], scope='conv1_2')\n",
    "net = slim.conv2d(net, 128, [3,3], scope='conv1_3')\n",
    "net = slim.conv2d(net, 128, [1,1], scope='conv1_4')\n",
    "net = slim.conv2d(net, 256, [3,3], scope='conv1_5')\n",
    "\n",
    "# equivalent:\n",
    "net = slim.stack(net, slim.conv2d, [(64, [3,3]), (64,[1,1]),\n",
    "                                    (128,[3,3]), (128,[1,1]),\n",
    "                                    (256,[3,3])], scope='con1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also have a scoping mechanism referred to as arg_scope , allowing users to\n",
    "pass a set of shared arguments to each operation defined in the same scope. Say, for\n",
    "example, that we have four layers having the same activation function, initialization,\n",
    "regularization, and padding. We can then simply use the slim.arg_scope command,\n",
    "where we specify the shared arguments as in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with slim.arg_scope([slim.conv2d], padding='VALID',\n",
    "                   activation_fn=tf.nn.relu,\n",
    "                   weights_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                   weights_regularizer=slim.l2_regularizer(0.0007)):\n",
    "    net = slim.conv2d(inputs, 64, [11,11], scope='conv1')\n",
    "    net = slim.conv2d(inputs, 128, [11,11], padding='VALID', scope='conv2')\n",
    "    net = slim.conv2d(inputs, 256, [11,11], scope='conv3')\n",
    "    net = slim.conv2d(inputs, 256, [11,11], scope='conv4')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual arguments inside the arg_scope command can still be overwritten,\n",
    "and we can also nest one arg_scope inside another.\n",
    "\n",
    "In these examples we used conv2d() : however, TF-Slim has many of the other stan‐\n",
    "dard methods for building neural networks. Table 7-4 lists some of the available\n",
    "options. For the full list, consult the documentation.\n",
    "\n",
    "BiasAdd:     slim.bias_add()\n",
    "\n",
    "BatchNorm:    slim.batch_norm()\n",
    "\n",
    "Conv2d:    slim.conv2d()\n",
    "\n",
    "Conv2dInPlane:    slim.conv2d_in_plane()\n",
    "\n",
    "Conv2dTranspose (Deconv):   slim.conv2d_transpose()\n",
    "\n",
    "FullyConnected:     slim.fully_connected()\n",
    "\n",
    "AvgPool2D :    slim.avg_pool2d()\n",
    "\n",
    "Dropout :    slim.dropout()\n",
    "\n",
    "Flatten  :   slim.flatten()\n",
    "\n",
    "MaxPool2D :   slim.max_pool2d()\n",
    "\n",
    "OneHotEncoding   :  slim.one_hot_encoding()\n",
    "\n",
    "SeparableConv2  :   slim.separable_conv2d()\n",
    "\n",
    "UnitNorm   :  slim.unit_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet\n",
    "To illustrate how convenient TF-Slim is for creating complex CNNs, we will build the\n",
    "VGG model by Karen Simonyan and Andrew Zisserman that was introduced in\n",
    "2014 (see the upcoming note for more information). VGG serves as a good illustra‐\n",
    "tion of how a model with many layers can be created compactly using TF-Slim. Here\n",
    "we construct the 16-layer version: 13 convolution layers plus 3 fully connected layers.\n",
    "\n",
    "Creating it, we take advantage of two of the features we’ve just mentioned:\n",
    "1. We use the arg_scope feature since all of the convolution layers have the same\n",
    "activation function and the same regularization and initialization.\n",
    "2. Many of the layers are exact duplicates of others, and therefore we also take\n",
    "advantage of the repeat command.\n",
    "\n",
    "The result very compelling—the entire model is defined with just 16 lines of code:\n",
    "\n",
    "### VGG and the ImageNet Challenge\n",
    "The ImageNet project is a large database of images collected for the\n",
    "purpose of researching visual object recognition. As of 2016 it con‐\n",
    "tained over 10 million hand-annotated images.\n",
    "Each year (since 2010) a competition takes place called the Image‐\n",
    "Net Large Scale Visual Recognition Challenge (ILSVRC), where\n",
    "research teams try to automatically classify, detect, and localize\n",
    "objects and scenes in a subset of the ImageNet collection. In the\n",
    "2012 challenge, dramatic progress occurred when a deep convolu‐\n",
    "tional neural net called AlexNet, created by Alex Krizhevsky, man‐\n",
    "aged to get a top 5 (top 5 chosen categories) classification error of\n",
    "only 15.4%, winning the competition by a large margin.\n",
    "Over the next couple of years the error rate kept falling, from\n",
    "ZFNet with 14.8% in 2013, to GoogLeNet (introducing the Incep‐\n",
    "tion module) with 6.7% in 2014, to ResNet with 3.6% in 2015. The\n",
    "Visual Geometry Group (VGG) was another CNN competitor in\n",
    "the 2014 competition that also achieved an impressive low error\n",
    "rate (7.3%). A lot of people prefer VGG over GoogLeNet because it\n",
    "has a nicer, simpler architecture.\n",
    "In VGG the only spatial dimensions used are very small 3×3 filters\n",
    "with a stride of 1 and a 2×2 max pooling, again with a stride of 1.\n",
    "Its superiority is achieved by the number of layers it uses, which is\n",
    "between 16 and 19.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-da287ec6368a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-da287ec6368a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    net =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                   activation_fn=tf.nn.relu,\n",
    "                   weights_initializer=tf.truncated.normal_initializer(0.0, 0.01),\n",
    "                   weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "    \n",
    "    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3,3], scope='con1')\n",
    "    net = slim.max_pool2d(net, [2,2], scope='pool1')\n",
    "    \n",
    "    net = slim.repeat(inputs, 2, slim.conv2d, 128, [3,3], scope='con2')\n",
    "    net = slim.max_pool2d(net, [2,2], scope='pool2')\n",
    "    \n",
    "    net = slim.repeat(inputs, 2, slim.conv2d, 256, [3,3], scope='con3')\n",
    "    net = slim.max_pool2d(net, [2,2], scope='pool3')\n",
    "\n",
    "    net = slim.repeat(inputs, 2, slim.conv2d, 512, [3,3], scope='con4')\n",
    "    net = slim.max_pool2d(net, [2,2], scope='pool4')\n",
    "\n",
    "    net = slim.repeat(inputs, 2, slim.conv2d, 512, [3,3], scope='con5')\n",
    "    net = slim.max_pool2d(net, [2,2], scope='pool5')\n",
    "\n",
    "    net = slim.fully_connected(net, 4096, scope='fc6')\n",
    "    net = slim.dropout(net,0.5, scope='dropout6')\n",
    "    \n",
    "    net = slim.fully_connected(net, 4096, scope='fc7')\n",
    "    net = slim.dropout(net,0.5, scope='dropout7')\n",
    "    \n",
    "    net = slim.fully_connected(net, 1000, scope='fc8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and using a pretrained model\n",
    "First we need to clone the repository where the actual models will reside by running:\n",
    "git clone https://github.com/tensorflow/models\n",
    "Now we have the scripts we need for modeling on our computer, and we can use\n",
    "them by setting the path:\n",
    "\n",
    "Next we will download the pretrained VGG-16 (16 layers) model—it is available on\n",
    "GitHub, as are other models, such as Inception, ResNet, and more:\n",
    "https://github.com/tensorflow/models/tree/master/research/slim\n",
    "The downloaded checkpoint file contains information about both the model and the\n",
    "variables. Now we want to load it and use it for classification of new images.\n",
    "\n",
    "Then we prepare our input image, turning it into a reada‐\n",
    "ble TensorFlow format and performing a little pre-processing to make sure that it is\n",
    "resized to match the size of the images the model was trained on.\n",
    "\n",
    "Now we create the model from the script we cloned earlier. We pass the model func‐\n",
    "tion the images and number of classes. The model has shared arguments; therefore,\n",
    "we call it using arg_scope , as we saw earlier, and use the vgg_arg_scope() function\n",
    "in the script to define the shared arguments. The function is shown in the following\n",
    "code snippet.\n",
    "\n",
    "vgg_16() returns the logits (numeric values acting as evidence for each class), which\n",
    "we can then turn into probabilities by using tf.nn.softmax() . We use the argument\n",
    "is_training to indicate that we are interested in forming predictions rather than\n",
    "training:\n",
    "\n",
    "Now, just before starting the session, we need to load the variables we downloaded\n",
    "using slim.assign_from_checkpoint_fn() , to which we pass the containing direc‐\n",
    "tory:\n",
    "\n",
    "Finally, the main event—we run the session, load the variables, and feed in the images\n",
    "and the desired probabilities.\n",
    "\n",
    "related resources:\n",
    "module import:\n",
    "https://stackoverflow.com/questions/4534438/typeerror-module-object-is-not-callable\n",
    "\n",
    "http://iot-fans.xyz/2017/11/30/deeplearning/classify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import slim\n",
    "import sys\n",
    "sys.path.append(\"/home/desktop/tensorflow/models/research/slim\")\n",
    "\n",
    "#from datasets. import dataset_utils\n",
    "from nets import vgg\n",
    "from preprocessing.vgg_preprocessing import preprocess_image\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#import urllib2\n",
    "\n",
    "target_dir = '/home/desktop/tensorflow/checkpoints'\n",
    "\n",
    "'''\n",
    "# For a URL link, we can load the image as a string with urllib2\n",
    "url=('http://54.68.5.226/car.jpg')\n",
    "im_as_string = urllib2.urlopen(url).read()\n",
    "\n",
    "im = tf.image.decode_jpeg(im_as_string, channels=3)\n",
    "# or for png\n",
    "im = tf.image.decode_png(im_as_string, channels=3)\n",
    "'''\n",
    "\n",
    "## for an image stored in our computer, we can create a queue of our filenames in the\n",
    "# target directory, and then read the entire image file by using tf.WholeFileReader() :\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    tf.train.match_filenames_once('/home/desktop/tensorflow/images/lakeside.png'))\n",
    "image_reader = tf.WholeFileReader()\n",
    "_, image_file = image_reader.read(filename_queue)\n",
    "\n",
    "image = tf.image.decode_png(image_file)\n",
    "\n",
    "image_size = vgg.vgg_16.default_image_size\n",
    "\n",
    "processed_im = preprocess_image(image, image_size,image_size)\n",
    "\n",
    "processed_images = tf.expand_dims(processed_im, 0)\n",
    "\n",
    "with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "    logits, _ = vgg.vgg_16(processed_images, num_classes=1000,\n",
    "                          is_training=False)\n",
    "    \n",
    "probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                       activation_fn=tf.nn.relu,\n",
    "                       weight_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                       biases_initializer=tf.zeros.initializer):\n",
    "        with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "            return arg_sc\n",
    "    \n",
    "load_vars = slim.assign_from_checkpoint_fn(os.path.join(target_dir, 'vgg_16.ckpt'),\n",
    "                                          slim.get_model_variables('vgg_16'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'datasets.dataset_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-010a7a764962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/desktop/tensorflow/models/research/slim\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_label_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'datasets.dataset_utils'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/desktop/tensorflow/checkpoints/vgg_16.ckpt\n"
     ]
    }
   ],
   "source": [
    "# We can get the class names by using the following lines:\n",
    "from datasets.imagenet import create_readable_names_for_imagenet_labels\n",
    "\n",
    "names = []\n",
    "with tf.Session() as sess:\n",
    "    load_vars(sess)\n",
    "    network_input, probabilities = sess.run([processed_images,\n",
    "                                            probabilities])\n",
    "    probabilities = probabilities[0, 0:]\n",
    "    names_ = create_readable_names_for_imagenet_labels()\n",
    "    idxs = np.argsort(-probabilities)[:5]\n",
    "    probs = probabilities[idxs]\n",
    "    classes = np.array(names_.values())[idxs+1]\n",
    "    \n",
    "    for c, p in zip(classes, probs):\n",
    "        print('class: ' + c + ' |prob: ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

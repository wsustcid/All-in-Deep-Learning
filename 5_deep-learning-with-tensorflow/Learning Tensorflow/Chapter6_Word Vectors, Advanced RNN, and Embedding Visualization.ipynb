{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Word Embeddings\n",
    "Consider the sentence appearing in Figure 6-1: “Our company provides smart agri‐\n",
    "culture solutions for farms, with advanced AI, deep-learning.” This sentence may be\n",
    "taken from, say, a tweet promoting a company. As data scientists or engineers, we\n",
    "now may wish to process it as part of an advanced machine intelligence system, that\n",
    "sifts through tweets and automatically detects informative content (e.g., public senti‐\n",
    "ment).\n",
    "\n",
    "In one of the major traditional natural language processing (NLP) approaches to text\n",
    "processing, each of the words in this sentence would be represented with N ID—say,\n",
    "an integer. So, as we posited in the previous chapter, the word “agriculture” might be\n",
    "mapped to the integer 3452, the word “farm” to 12, “AI” to 150, and “deep-learning”\n",
    "to 0.\n",
    "\n",
    "While this representation has led to excellent results in practice in some basic NLP\n",
    "tasks and is still often used in many cases (such as in bag-of-words text classification),\n",
    "it has some major inherent problems. First, by using this type of atomic representa‐\n",
    "tion, we lose all meaning encoded within the word, and crucially, we thus lose infor‐\n",
    "mation on the semantic proximity between words. In our example, we of course\n",
    "know that “agriculture” and “farm” are strongly related, and so are “AI” and “deep-\n",
    "learning,” while deep learning and farms don’t usually have much to do with one\n",
    "another. This is not reflected by their arbitrary integer IDs.\n",
    "\n",
    "Another important issue with this way of looking at data stems from the size of typi‐\n",
    "cal vocabularies, which can easily reach huge numbers. This means that naively, we\n",
    "could need to keep millions of such word identifiers, leading to great data sparsity\n",
    "and in turn, making learning harder and more expensive.\n",
    "\n",
    "With images, such as in the MNIST data we used in the first section of Chapter 5, this\n",
    "is not quite the case. While images can be high-dimensional, their natural representa‐\n",
    "tion in terms of pixel values already encodes some semantic meaning, and this repre‐\n",
    "sentation is dense. In practice, RNN models like the one we saw in Chapter 5 require\n",
    "dense vector representations to work well.\n",
    "\n",
    "We would like, therefore, to use dense vector representations of words, which carry\n",
    "semantic meaning. But how do we obtain them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2vec\n",
    "Word2vec is a very well-known unsupervised word embedding approach. It is\n",
    "actually more like a family of algorithms, all based in some way on exploiting the\n",
    "context in which words appear to learn their representation (in the spirit of the distri‐\n",
    "butional hypothesis). We focus on the most popular word2vec implementation,\n",
    "which trains a model that, given an input word, predicts the word’s context by using\n",
    "something known as skip-grams. This is actually rather simple, as the following exam‐\n",
    "ple will demonstrate.\n",
    "\n",
    "Consider, again, our example sentence: “Our company provides smart agriculture sol‐\n",
    "utions for farms, with advanced AI, deep-learning.” We define (for simplicity) the\n",
    "context of a word as its immediate neighbors (“the company it keeps”)—i.e., the word\n",
    "to its left and the word to its right. So, the context of “company” is [our, provides], the context of “AI” is [advanced, deep-learning], and so on (see Figure 6-1).\n",
    "\n",
    "In the skip-gram word2vec model, we train a model to predict context based on an\n",
    "input word. All that means in this case is that we generate training instance and label\n",
    "pairs such as (our, company), (provides, company), (advanced, AI), (deep-learning,\n",
    "AI), etc.\n",
    "\n",
    "In addition to these pairs we extract from the data, we also sample “fake” pairs—that\n",
    "is, for a given input word (such as “AI”), we also sample random noise words as con‐\n",
    "text (such as “monkeys”), in a process known as negative sampling. We use the true\n",
    "pairs combined with noise pairs to build our training instances and labels, which we\n",
    "use to train a binary classifier that learns to distinguish between them. The trainable\n",
    "parameters in this classifier are the vector representations—word embeddings. We\n",
    "tune these vectors to yield a classifier able to tell the difference between true contexts\n",
    "of a word and randomly sampled ones, in a binary classification setting.\n",
    "\n",
    "\n",
    "### Skip-Grams\n",
    "We begin by preparing our data and extracting skip-grams. As in Chapter 5, our data\n",
    "comprises two classes of very short “sentences,” one composed of odd digits and the\n",
    "other of even digits (with numbers written in English). We make sentences equally\n",
    "sized here, for simplicity, but this doesn’t really matter for word2vec training. Let’s\n",
    "start by setting some parameters and creating sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "batch_size = 64 \n",
    "embedding_dimension = 5 \n",
    "negative_samples = 8 \n",
    "log_dir = \"logs/word2vec_intro\"\n",
    "\n",
    "# the digit here has no special meaning, just for creating sentence data \n",
    "digit_to_word_map = {1: \"One\", 2: \"Two\", 3: \"Three\",\n",
    "                    4: \"Four\", 5: \"Five\", 6: \"Six\",\n",
    "                    7: \"Seven\", 8: \"Eight\", 9: \"Nine\"}\n",
    "\n",
    "sentences = []\n",
    "\n",
    "# create two kinds of sentences - seqences of odd and even digits\n",
    "for i in range(10000):\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), 3)\n",
    "    sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    rand_even_ints = np.random.choice(range(2,10,2), 3)\n",
    "    sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 \n",
      " \n",
      " ['Nine Seven Nine', 'Six Six Six', 'One One Three', 'Eight Six Eight', 'Nine Three Nine', 'Four Four Eight', 'Nine Nine Nine', 'Six Four Two', 'Five Nine One', 'Four Four Two']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), \"\\n \\n\", sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## as in Chapter 5, we map words to indices by creating a dictionary \n",
    "## with words as keys and indices as values, and create the inverse map\n",
    "\n",
    "word2index_map = {}\n",
    "index = 0 \n",
    "for sent in sentences:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_size, len(word2index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to prepare the data for word2vec, let's create skip-grams(语义相关对)\n",
    "skip_gram_pairs = []\n",
    "for sent in sentences:\n",
    "    tokenized_sent = sent.lower().split()\n",
    "    for i in range(1, len(tokenized_sent)-1): ## i=1\n",
    "        # [[0,2],1]\n",
    "        word_context_pair = [[word2index_map[tokenized_sent[i-1]],\n",
    "                              word2index_map[tokenized_sent[i+1]]],\n",
    "                             word2index_map[tokenized_sent[i]]\n",
    "                            ]\n",
    "        \n",
    "        # [1, 0]\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                               word_context_pair[0][0]])\n",
    "        # [1, 2]\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                               word_context_pair[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0],\n",
       " [1, 0],\n",
       " [2, 2],\n",
       " [2, 2],\n",
       " [3, 3],\n",
       " [3, 4],\n",
       " [2, 5],\n",
       " [2, 5],\n",
       " [4, 0],\n",
       " [4, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Each skip-gram pair is composed of target and context word indices (given by the\n",
    "word2index_map dictionary, and not in correspondence to the actual digit each word\n",
    "represents). Let’s take a look:\n",
    "'''\n",
    "skip_gram_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram_batch(batch_size):\n",
    "    instance_indices = list(range(len(skip_gram_pairs)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    \n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [skip_gram_pairs[i][0] for i in batch] # (batch,)\n",
    "    y = [[skip_gram_pairs[i][1]] for i in batch] ## (batch,1)\n",
    "    \n",
    "    # x,y is a pair of semantically related word\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 1, 5, 8, 1, 2, 1, 5] \n",
      "\n",
      "[[5], [0], [2], [1], [4], [2], [4], [7]] \n",
      "\n",
      "['two', 'seven', 'eight', 'five', 'seven', 'six', 'seven', 'eight'] \n",
      "\n",
      "['eight', 'nine', 'six', 'seven', 'three', 'six', 'three', 'two']\n"
     ]
    }
   ],
   "source": [
    "# We can generate batches of sequences of word indices, and check out \n",
    "# the original sentences with the inverse dictionary we created earlier:\n",
    "\n",
    "# batch example\n",
    "x_batch, y_batch = get_skipgram_batch(8)\n",
    "print(x_batch, \"\\n\")\n",
    "print(y_batch, \"\\n\")\n",
    "print([index2word_map[index] for index in x_batch], '\\n')\n",
    "print([index2word_map[index[0]] for index in y_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input and label placeholders\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding in TensorFlow\n",
    "In Chapter 5, we used the built-in tf.nn.embedding_lookup() function as part of\n",
    "our supervised RNN. The same functionality is used here. Here too, word embed‐\n",
    "dings can be viewed as lookup tables that **map words to vector values**, which are opti‐\n",
    "mized as part of the training process to minimize a loss function. As we shall see in\n",
    "the next section, unlike in Chapter 5, here we use a loss function accounting for the\n",
    "unsupervised nature of the task, but the embedding lookup, which efficiently\n",
    "retrieves the vectors for each word in a given sequence of word indices, remains the\n",
    "same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embeddings\"):\n",
    "    # (9,5)\n",
    "    embeddings = tf.Variable(tf.random_uniform(\n",
    "        [vocabulary_size, embedding_dimension],-1.0, 1.0), \n",
    "                             name='embedding')\n",
    "    # this is essentially a lookup table\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Noise-Contrastive Estimation (NCE) Loss Function\n",
    "In our introduction to skip-grams, we mentioned we create two types of context–\n",
    "target pairs of words: real ones that appear in the text, and “fake” noisy pairs that are\n",
    "generated by inserting random context words. Our goal is to learn to distinguish\n",
    "between the two, helping us learn a good word representation. We could draw ran‐\n",
    "dom noisy context pairs ourselves, but luckily TensorFlow comes with a useful loss\n",
    "function designed especially for our task. tf.nn.nce_loss() automatically draws\n",
    "negative (“noise”) samples when we evaluate the loss (run it in a session):\n",
    "\n",
    "We don’t go into the mathematical details of this loss function, but it is sufficient to\n",
    "think of it as a sort of efficient approximation to the ordinary softmax function used\n",
    "in classification tasks, as introduced in previous chapters. We tune our embedding\n",
    "vectors to optimize this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'NCE_loss_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create variables for the NCE loss\n",
    "nce_weights = tf.Variable(tf.truncated_normal(\n",
    "    [vocabulary_size, embedding_dimension],\n",
    "    stddev=1.0/math.sqrt(embedding_dimension)))\n",
    "\n",
    "nce_bias = tf.Variable(tf.zeros([vocabulary_size])) \n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights, biases=nce_bias,\n",
    "                  inputs=embed, labels=train_labels, \n",
    "                   num_sampled=negative_samples,\n",
    "                  num_classes=vocabulary_size))\n",
    "tf.summary.scalar(\"NCE_loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Decay\n",
    "As discussed in previous chapters, gradient-descent optimization adjusts weights by\n",
    "making small steps in the direction that minimizes our loss function. The learn\n",
    "ing_rate hyperparameter controls just how aggressive these steps are. During\n",
    "gradient-descent training of a model, it is common practice to gradually make these\n",
    "steps smaller and smaller, so that we allow our optimization process to “settle down”\n",
    "as it approaches good points in the parameter space. This small addition to our train\n",
    "ing process can actually often lead to significant boosts in performance, and is a good\n",
    "practice to keep in mind in general.\n",
    "\n",
    "tf.train.exponential_decay() applies exponential decay to the learning rate, with\n",
    "the exact form of decay controlled by a few hyperparameters, as seen in the following\n",
    "code (for exact details, see the official TensorFlow documentation at http://bit.ly/\n",
    "2tluxP1). Here, just as an example, we decay every 1,000 steps, and the decayed learn‐\n",
    "ing rate follows a staircase function—a piecewise constant function that resembles a\n",
    "staircase, as its name implies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.1,\n",
    "                                         global_step=global_step,\n",
    "                                         decay_steps=1000,\n",
    "                                         decay_rate=0.95,\n",
    "                                         staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learningRate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Visualizing with TensorBoard\n",
    "We train our graph within a session as usual, adding some lines of code enabling cool\n",
    "interactive visualization in TensorBoard, a new tool for visualizing embeddings of\n",
    "high-dimensional data—typically images or word vectors—introduced for Tensor‐\n",
    "Flow in late 2016.\n",
    "First, we create a TSV (tab-separated values) metadata file. This file connects embed‐\n",
    "ding vectors with associated labels or images we may have for them. In our case, each\n",
    "embedding vector has a label that is just the word it stands for.\n",
    "We then point TensorBoard to our embedding variables (in this case, only one), and\n",
    "link them to the metadata file.\n",
    "Finally, after completing optimization but before closing the session, we normalize\n",
    "the word embedding vectors to unit length, a standard post-processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 6.87755\n",
      "100 loss: 3.05456\n",
      "200 loss: 2.76139\n",
      "300 loss: 2.63624\n",
      "400 loss: 2.59729\n",
      "500 loss: 2.60475\n",
      "600 loss: 2.42422\n",
      "700 loss: 2.51470\n",
      "800 loss: 2.53672\n",
      "900 loss: 2.42953\n",
      "WARNING:tensorflow:From <ipython-input-15-5bb912f25cdd>:42: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# merge all summary ops\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(log_dir,\n",
    "                                        graph=tf.get_default_graph())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with open(os.path.join(log_dir,'metadata.tsv'), 'w') as metadata:\n",
    "        metadata.write('Name\\tClass\\n')\n",
    "        for k,v in index2word_map.items():\n",
    "            metadata.write('%s\\t%d\\n' % (v,k))\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embeddings.name\n",
    "    \n",
    "    # Link embedding to its metadata file\n",
    "    embedding.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch = get_skipgram_batch(batch_size)\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=\n",
    "                              {train_inputs:x_batch,\n",
    "                              train_labels:y_batch})\n",
    "        \n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            saver.save(sess, os.path.join(log_dir,'w2v_model.ckpt'), step)\n",
    "            loss_value = sess.run(loss,  feed_dict=\n",
    "                              {train_inputs:x_batch,\n",
    "                              train_labels:y_batch})\n",
    "            print(\"{} loss: {:.5f}\".format(step, loss_value))\n",
    "            \n",
    "\n",
    "    # Nomalize embedding before using\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1,keep_dims=True))\n",
    "    normlized_embeddings = embeddings/norm\n",
    "    normlized_embeddings_matrix = sess.run(normlized_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Out Our Embeddings\n",
    "Let’s take a quick look at the word vectors we got. We select one word (one) and sort\n",
    "all the other word vectors by how close they are to it, in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01562983,  0.23738617,  0.6803132 , -0.36496282,  0.58938915],\n",
       "       [-0.03093336,  0.5747406 ,  0.44531378, -0.47939005,  0.49050716],\n",
       "       [-0.13070045,  0.2055766 , -0.5406117 ,  0.5769654 ,  0.5616989 ],\n",
       "       [-0.4513124 ,  0.4811691 ,  0.19745803, -0.5241226 ,  0.5010981 ],\n",
       "       [-0.26016575, -0.2842037 ,  0.7569622 , -0.06624044,  0.5236052 ],\n",
       "       [ 0.58672523,  0.18213524, -0.15647183,  0.61974293,  0.46261805],\n",
       "       [ 0.05070774, -0.1732344 , -0.28025463,  0.71253866,  0.6173852 ],\n",
       "       [-0.12822342,  0.18397172, -0.54695904,  0.55820584,  0.5821986 ],\n",
       "       [ 0.11286665,  0.47170553,  0.5797224 , -0.42758858,  0.49582753]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (vocabulary_size, embedding_dimension)\n",
    "normlized_embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven\n",
      "0.8754901\n",
      "five\n",
      "0.76307\n",
      "nine\n",
      "0.7281293\n",
      "three\n",
      "0.42722994\n",
      "two\n",
      "0.03755921\n",
      "six\n",
      "0.030221403\n",
      "four\n",
      "-0.22566566\n",
      "eight\n",
      "-0.30105942\n"
     ]
    }
   ],
   "source": [
    "ref_word = normlized_embeddings_matrix[word2index_map[\"one\"]] # (5,)\n",
    "\n",
    "cosine_dists = np.dot(normlized_embeddings_matrix, ref_word) # (9,)\n",
    "ff = np.argsort(cosine_dists)[::-1][1:10]\n",
    "for f in ff:\n",
    "    print(index2word_map[f])\n",
    "    print(cosine_dists[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the word vectors representing odd numbers are similar (in terms of the\n",
    "dot product) to one, while those representing even numbers are not similar to it (and\n",
    "have a negative dot product with the one vector). We learned embedded vectors that\n",
    "allow us to distinguish between even and odd numbers—their respective vectors are\n",
    "far apart, and thus capture the context in which each word (odd or even\n",
    "digit) appeared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "Now, in TensorBoard, go to the Embeddings tab. This is a three-dimensional interac‐\n",
    "tive visualization panel, where we can move around the space of our embedded vec‐\n",
    "tors and explore different “angles,” zoom in, and more (see Figures 6-2 and 6-3). This\n",
    "enables us to understand our data and interpret the model in a visually comfortable\n",
    "manner. We can see, for instance, that the odd and even numbers occupy different\n",
    "areas in feature space.\n",
    "\n",
    "~/tensorflow/examples/logs$ tensorboard --logdir=word2vec_intro\n",
    "\n",
    "TensorBoard 1.9.0 at http://desktop:6006 (Press CTRL+C to quit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pretrained Embeddings, Advanced RNN\n",
    "\n",
    "Here, we show how to take word vectors trained based on web data and incorporate\n",
    "them into a (contrived) text-classification task. The embedding method is known as\n",
    "GloVe, and while we don’t go into the details here, the overall idea is similar to that of\n",
    "word2vec—learning representations of words by the context in which they appear.\n",
    "Information on the method and its authors, and the pretrained vectors, is available on\n",
    "the project’s website.(https://nlp.stanford.edu/projects/glove/)\n",
    "We download the Common Crawl vectors (840B tokens), and proceed to our exam‐\n",
    "ple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "path_to_glove = \"glove/glove.840B.300d.zip\"\n",
    "PRE_TRAINED = True\n",
    "GLOVE_SIZE = 300\n",
    "batch_size =128\n",
    "emdedding_dimension = 64 \n",
    "num_classes = 2 \n",
    "hidden_layer_size = 32 \n",
    "times_steps = 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the contrived, simple simulated data\n",
    "digit_to_word_map = {1:\"One\", 2:\"Two\", 3:\"Three\",\n",
    "                    4:\"Four\", 5:\"Five\",6:\"Six\",\n",
    "                    7:\"Seven\", 8:\"Eight\",9:\"Nine\"}\n",
    "\n",
    "digit_to_word_map[0] = \"PAD_TOKEN\"\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    \n",
    "    rand_odd_ints = np.random.choice(range(1,10,2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2), rand_seq_len)\n",
    "    \n",
    "    if rand_seq_len < 6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                 [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                 [0]*(6-rand_seq_len))\n",
    "        \n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    \n",
    "data = even_sentences + odd_sentences\n",
    "# same seq lengths for even, odd sentences\n",
    "seqlens*=2 \n",
    "\n",
    "labels = [1]*10000 + [0]*10000 \n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1 # the true class label is 1\n",
    "    labels[i] = one_hot_encoding\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(len(seqlens))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the word index map\n",
    "word2index_map={}\n",
    "index = 0 \n",
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index +=1\n",
    "    \n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Eight': 0,\n",
       " 'Five': 5,\n",
       " 'Four': 2,\n",
       " 'Nine': 9,\n",
       " 'One': 7,\n",
       " 'PAD_TOKEN': 4,\n",
       " 'Seven': 8,\n",
       " 'Six': 1,\n",
       " 'Three': 6,\n",
       " 'Two': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "Two\n",
      "Three\n",
      "Four\n",
      "Five\n",
      "Six\n",
      "Seven\n",
      "Nine\n",
      "Eight\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now, we are ready to get word vectors. There are 2.2 million words in the vocabulary\n",
    "of the pretrained GloVe embeddings we downloaded, and in our toy example we have\n",
    "only 9. So, we take the GloVe vectors only for words that appear in our own tiny\n",
    "vocabulary\n",
    "\n",
    "We go over the GloVe file line by line, take the word vectors we need, and normalize\n",
    "them. Once we have extracted the nine words we need, we stop the process and exit\n",
    "the loop. The output of our function is a dictionary, mapping from each word to its\n",
    "vector.\n",
    "'''\n",
    "def get_glove(path_to_glove, word2index_map):\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0 \n",
    "    with zipfile.ZipFile(path_to_glove) as z:\n",
    "        with z.open(\"glove.840B.300d.txt\") as f:\n",
    "            for line in f: \n",
    "                vals = line.split()\n",
    "                word = str(vals[0].decode(\"utf-8\"))\n",
    "                if word in word2index_map:\n",
    "                    print(word)\n",
    "                    count_all_words+=1 \n",
    "                    coefs = np.asarray(vals[1:], dtype='float32')\n",
    "                    coefs /= np.linalg.norm(coefs)\n",
    "                    embedding_weights[word] = coefs\n",
    "                if count_all_words == vocabulary_size-1: # other one is PAD\n",
    "                    break\n",
    "    \n",
    "    return embedding_weights\n",
    "\n",
    "word2embedding_dict = get_glove(path_to_glove, word2index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to place these vectors in a matrix, which is the required format \n",
    "# for TensorFlow. In this matrix, each row index should correspond to the word index\n",
    "\n",
    "'''\n",
    "Note that for the PAD_TOKEN word, we set the corresponding vector to 0. As we saw in\n",
    "Chapter 5, we ignore padded tokens in our call to dynamic_rnn() by telling it the\n",
    "original sequence length.\n",
    "'''\n",
    "embedding_matrix = np.zeros((vocabulary_size, GLOVE_SIZE))\n",
    "\n",
    "for word, index in word2index_map.items():\n",
    "    if not word == \"PAD_TOKEN\":\n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n",
      "4\n",
      "[-7.93781411e-03  1.66579001e-02 -3.77636105e-02 -1.03353918e-01\n",
      "  4.17818828e-03 -7.74870813e-02 -9.07040667e-03  3.04094777e-02\n",
      "  3.47289853e-02  2.02921987e-01  1.27063785e-02  4.47718834e-04\n",
      "  8.15555602e-02  1.33160930e-02  6.10922948e-02 -9.28591713e-02\n",
      " -7.42144436e-02  1.43477172e-01  2.78657451e-02  2.21851207e-02\n",
      " -8.26582164e-02  1.72832329e-02  3.12927179e-02  3.99726368e-02\n",
      "  2.02736054e-02 -3.37713659e-02 -8.34001377e-02  1.66681278e-02\n",
      "  6.78421035e-02  1.57802384e-02 -2.86151059e-02  9.44136679e-02\n",
      "  1.38374837e-02  2.39497423e-02  5.37455976e-02 -1.35210045e-02\n",
      "  1.27777821e-04 -8.10609460e-02 -8.02111719e-03 -2.35592555e-02\n",
      " -2.34662853e-02 -2.01174114e-02 -8.85006227e-03 -1.17416959e-02\n",
      " -6.41529541e-03 -5.35540767e-02  2.50561144e-02  7.04471990e-02\n",
      " -4.72170636e-02  5.68899326e-03 -3.04541048e-02  6.87904209e-02\n",
      "  1.80418901e-02 -6.64047478e-03 -1.42391250e-01  6.53374195e-02\n",
      " -5.36340335e-03  8.69126394e-02  3.90336141e-02  6.96606487e-02\n",
      "  9.69759896e-02  9.60090850e-03  1.05317496e-01  9.20912158e-03\n",
      " -6.66501969e-02 -1.81653574e-02 -1.46744233e-02  2.19657049e-02\n",
      "  1.94591638e-02  1.08591989e-01  5.28995469e-02 -8.07299614e-02\n",
      "  6.72173277e-02 -1.08409766e-02  2.76221596e-02 -7.75484443e-02\n",
      " -6.09807260e-02 -6.06627613e-02  4.93796058e-02 -3.21164541e-02\n",
      " -3.72448228e-02  7.55681284e-03 -2.74548084e-02 -1.81603376e-02\n",
      " -9.36791822e-02  6.06534630e-02 -2.89126188e-01  1.44918248e-01\n",
      " -5.30650392e-02  6.68696128e-03 -8.78702551e-02  1.69753097e-02\n",
      " -3.01268399e-02 -2.19099224e-02  1.44150283e-03 -3.93013731e-02\n",
      "  3.05303410e-02 -3.37471925e-02 -4.90932502e-02 -5.98483160e-02\n",
      " -7.09473938e-02  1.04832180e-01  5.94578348e-02  4.28919792e-02\n",
      " -2.00095624e-02  2.80405343e-01  2.31650518e-03  6.56163394e-02\n",
      "  2.74641048e-02  8.58769268e-02  5.67988195e-02 -2.42788624e-02\n",
      "  1.19242936e-01  1.32231200e-02  2.93179806e-02 -3.44482064e-02\n",
      " -3.95170748e-02  1.65539563e-01 -7.32233599e-02  6.27750978e-02\n",
      "  1.28943687e-02 -4.82546389e-02 -2.86746081e-02  9.77681205e-03\n",
      " -5.97869605e-02 -6.67691976e-02 -2.43272111e-02 -4.43088785e-02\n",
      " -2.54447404e-02 -1.02262413e-02 -8.53655767e-03  4.04932834e-02\n",
      "  8.87851119e-02  8.09698366e-03 -3.25980526e-03  8.32736939e-02\n",
      "  3.36188916e-03  5.28047197e-02  3.12387925e-02  4.16907705e-02\n",
      " -9.41552073e-02 -2.18225289e-02  4.93944809e-03  1.15092639e-02\n",
      " -2.83231717e-02 -2.48367004e-02 -2.12907251e-02  1.61826257e-02\n",
      " -3.22577730e-02 -6.36118501e-02 -2.72837380e-04  3.12499497e-02\n",
      " -5.93388267e-03  6.16371110e-02  4.35725376e-02 -4.37529050e-02\n",
      " -1.27867069e-02  8.07727352e-02 -2.79624388e-03  5.93425473e-03\n",
      "  3.82563635e-03 -4.48016375e-02  1.32755572e-02 -2.11233739e-03\n",
      " -2.30330303e-02 -3.01305577e-02  2.46414565e-03 -5.11907116e-02\n",
      " -2.64637209e-02  8.25838372e-03 -2.74919979e-02 -3.67780998e-02\n",
      "  1.54327080e-02  3.95114943e-02 -6.71503842e-02 -5.03539592e-02\n",
      "  8.69870186e-02 -1.35083599e-02  9.77495313e-02 -1.00389950e-01\n",
      " -5.32918908e-02 -4.83792182e-03  5.11516668e-02 -4.04728316e-02\n",
      " -6.20220192e-02 -1.96060613e-02  2.06250395e-03  2.55116802e-02\n",
      "  8.39765649e-03  2.88159270e-02  4.96045984e-02  1.44286035e-02\n",
      "  4.41080593e-02  6.96439156e-03 -6.59064129e-02 -3.40168132e-03\n",
      " -6.40990287e-02 -8.90844781e-03 -5.86843044e-02 -4.99430187e-02\n",
      "  4.17428371e-03 -2.92138513e-02 -2.20028944e-02  6.11722507e-02\n",
      " -2.49110768e-03 -1.98924169e-02 -8.02018866e-02 -3.09840478e-02\n",
      "  2.78508719e-02  3.01975012e-03  4.65253443e-02  6.62541315e-02\n",
      " -9.09755453e-02 -5.81506416e-02 -4.57127653e-02  3.01789050e-03\n",
      " -1.19705945e-02 -1.51906069e-02  3.68059911e-02 -1.65121183e-02\n",
      " -4.20440659e-02  1.37746343e-02 -2.30348911e-02  6.86342269e-02\n",
      "  2.42751446e-02 -7.24944519e-03 -1.14456704e-02  3.75274606e-02\n",
      " -3.08222752e-02  2.10415572e-02 -7.66075552e-02  4.42921445e-02\n",
      "  3.24009508e-02  7.06610382e-02 -1.50444536e-02  6.26467960e-03\n",
      " -1.40330978e-02 -2.10378394e-02  4.40876074e-02  5.90785071e-02\n",
      " -2.24584602e-02 -3.69064026e-02 -2.06566509e-02  5.15960753e-02\n",
      " -3.24139670e-02  3.49539779e-02 -3.46025415e-02  2.86597330e-02\n",
      " -9.25207511e-03  6.46196753e-02 -5.20330444e-02 -1.91951226e-02\n",
      "  5.45005361e-03 -5.14268614e-02 -7.11426362e-02  1.17695875e-01\n",
      " -5.83086945e-02  1.54591110e-02 -1.42275961e-02  3.15418839e-02\n",
      "  7.22843334e-02 -6.08449839e-02 -1.50963321e-01  1.11037178e-03\n",
      "  2.90223248e-02 -6.93668574e-02  4.51605096e-02  5.87921478e-02\n",
      " -7.57447779e-02 -7.82327205e-02  6.27230331e-02 -8.22193846e-02\n",
      " -1.00161240e-01 -7.29853511e-02  1.66322384e-03  7.04416186e-02\n",
      "  7.49935582e-02  3.75832431e-02  2.78824810e-02  1.22117642e-02\n",
      "  1.33951204e-02 -3.77877825e-03 -1.16089299e-01 -3.54318582e-02\n",
      " -2.14413404e-02  8.19479078e-02 -1.42415427e-02  1.16866559e-03\n",
      " -7.20333084e-02  4.86283861e-02 -6.28662109e-02 -3.37992571e-02\n",
      " -6.90526068e-02  4.62538674e-02 -7.99155235e-02 -1.89850037e-03\n",
      "  7.61371199e-03 -4.94539849e-02  6.18212000e-02 -5.90450354e-02]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(word2index_map[\"PAD_TOKEN\"])\n",
    "print(embedding_matrix[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create our training and test data\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = data[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "def get_sentence_batch(batch_size, data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    \n",
    "    x = [[word2index_map[word] for word in data_x[i].split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    \n",
    "    return x, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input placeholders\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size, times_steps])\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, GLOVE_SIZE])\n",
    "\n",
    "_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Our embeddings are initialized with the content of embedding_placeholder , using\n",
    "the assign() function to assign initial values to the embeddings variable. We set\n",
    "trainable=True to tell TensorFlow we want to update the values of the word vectors,\n",
    "by optimizing them for the task at hand. However, it is often useful to set\n",
    "trainable=False and not update these values; for example, when we do not have\n",
    "much labeled data or have reason to believe the word vectors are already “good” at\n",
    "capturing the patterns we are after.\n",
    "'''\n",
    "# we created an embedding_placeholder , to which we feed the word vectors\n",
    "if PRE_TRAINED:\n",
    "    embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, \n",
    "                                                    GLOVE_SIZE]),\n",
    "                            trainable=True)\n",
    "    \n",
    "    # if using pretrained embeddings, assign them to the embedding variable\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "    \n",
    "else:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, \n",
    "                                                embedding_dimension],\n",
    "                                              -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN and GRU Cells\n",
    "Bidirectional RNN layers are a simple extension of the RNN layers we saw in Chap‐\n",
    "ter 5. All they consist of, in their basic form, is two ordinary RNN layers: one layer\n",
    "that reads the sequence from left to right, and another that reads from right to left.\n",
    "Each yields a hidden representation, the left-to-right vector h , and the right-to-left\n",
    "vector h . These are then concatenated into one vector. The major advantage of this\n",
    "representation is its ability to capture the context of words from both directions,\n",
    "which enables richer understanding of natural language and the underlying seman‐\n",
    "tics in text.\n",
    "\n",
    "**Gated recurrent unit (GRU) cells are a simplification of sorts of LSTM cells. They also\n",
    "have a memory mechanism, but with considerably fewer parameters than LSTM.\n",
    "They are often used when there is less available data, and are faster to compute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TensorFlow comes equipped with tf.nn.bidirectional_dynamic_rnn() , which is\n",
    "an extension of dynamic_rnn() for bidirectional layers. It takes cell_fw and cell_bw\n",
    "RNN cells, which are the left-to-right and right-to-left vectors, respectively. Here we\n",
    "use GRUCell() for our forward and backward representations and add dropout for\n",
    "regularization, using the built-in DropoutWrapper() :\n",
    "'''\n",
    "with tf.name_scope(\"biGRU\"):\n",
    "    with tf.variable_scope(\"forward\"):\n",
    "        gru_fw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell)\n",
    "        \n",
    "    with tf.variable_scope(\"backward\"):\n",
    "        gru_bw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n",
    "        gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell)\n",
    "        \n",
    "    \n",
    "    outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell,\n",
    "                                                      cell_bw=gru_bw_cell,\n",
    "                                                      inputs=embed,\n",
    "                                                     sequence_length=_seqlens,\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     scope=\"BiGRU\")\n",
    "states = tf.concat(values=states, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-b82526a466eb>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-13-b82526a466eb>:19: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "# We concatenate the forward and backward state vectors by using tf.concat() along\n",
    "# the suitable axis, and then add a linear layer followed by softmax\n",
    "weights = {'linear_layer': tf.Variable(tf.truncated_normal([2*hidden_layer_size,\n",
    "                                                           num_classes],\n",
    "                                                          mean=0, stddev=0.01))}\n",
    "\n",
    "biases = {'linear_layer': tf.Variable(tf.truncated_normal([num_classes],\n",
    "                                                          mean=0, stddev=0.01))}\n",
    "\n",
    "# extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states, weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=final_output, labels=_labels))\n",
    "\n",
    "train_step = tf.train.RMSPropOptimizer(0.01,0.9).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1),\n",
    "                              tf.arg_max(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train acc: 46.0938%\n",
      "100 train acc: 100.0000%\n",
      "200 train acc: 100.0000%\n",
      "300 train acc: 100.0000%\n",
      "400 train acc: 100.0000%\n",
      "500 train acc: 100.0000%\n",
      "600 train acc: 100.0000%\n",
      "700 train acc: 100.0000%\n",
      "800 train acc: 100.0000%\n",
      "900 train acc: 100.0000%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'One Nine Nine PAD_TOKEN PAD_TOKEN PAD_TOKEN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-52467f123a55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         batch_acc = sess.run(accuracy, feed_dict={_inputs:x_test,\n\u001b[1;32m     33\u001b[0m                                                     \u001b[0m_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                                     _seqlens:seqlen_test})\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} test acc: {:.4f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/tensorflow/venv/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'One Nine Nine PAD_TOKEN PAD_TOKEN PAD_TOKEN'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We are now ready to train. We initialize the embedding_placeholder by feeding it\n",
    "our embedding_matrix . It’s important to note that we do so after calling\n",
    "tf.global_variables_initializer() —doing this in the reverse order would over‐\n",
    "run the pre-trained vectors with a default initializer:\n",
    "'''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder:embedding_matrix})\n",
    "    \n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                                           train_x,\n",
    "                                                           train_y,\n",
    "                                                           train_seqlens)\n",
    "        sess.run(train_step, feed_dict={_inputs:x_batch,\n",
    "                                        _labels:y_batch,\n",
    "                                        _seqlens:seqlen_batch})\n",
    "        if step % 100 ==0:\n",
    "            acc = sess.run(accuracy, feed_dict={_inputs:x_batch,\n",
    "                                        _labels:y_batch,\n",
    "                                        _seqlens:seqlen_batch})\n",
    "            print(\"{} train acc: {:.4f}%\".format(step, acc))\n",
    "            \n",
    "            \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                           test_x,\n",
    "                                                           test_y,\n",
    "                                                           test_seqlens)\n",
    "        \n",
    "        batch_acc = sess.run(accuracy, feed_dict={_inputs:x_test,\n",
    "                                                    _labels:y_test,\n",
    "                                                    _seqlens:seqlen_test})\n",
    "        \n",
    "        print(\"{} test acc: {:.4f}%\".format(test_batch, batch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

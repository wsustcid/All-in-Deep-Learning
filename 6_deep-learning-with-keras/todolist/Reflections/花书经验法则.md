## 半天2k赞火爆推特！李飞飞高徒发布33条神经网络训练秘技

关注前沿科技 [量子位](javascript:void(0);) *4月26日*

##### 栗子 晓查 编译 量子位 出品 | 公众号 QbitAI

![img](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCQYLj62wpY5xicKlLfDCpKVqdzHS3HUTGLUFZJLMuyMmdlZ46wwGbzuJcRRGtJ4X8MfLVHiaXSsEDQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

同学，现在有一份33条神经网络训练秘笈，摆在你面前。

AI大佬**Andrej Karpathy** (简称AK) ，刚刚发布了一篇长长长长博客，苦口婆心地列举了33条技巧和注意事项，全面避免大家踩坑，推特已有**2,300**多赞。

![img](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCQYLj62wpY5xicKlLfDCpKVgia8U1TY7brSjdlHh73ydXicCqPLICf9jLZlsOLLL396AYZmsMhbicObg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

AK在斯坦福读博的时候，是飞飞实验室的成员，毕业去了OpenAI，然后又成了特斯拉的AI负责人，直到如今。

他的博客虽然一年一更，但一字一句皆是皆是多年心血凝结而成，每次更新必有重大回响。

有生之年，我们把内文翻译如下：

## 训练模型的“处方”

总的来说，Andrej Karpathy的技巧就是：不要心急 (**文章结尾会道出原因**) ，从简单到复杂逐步完善你的神经网络。

### 1、先别着急写代码

训练神经网络前，别管代码，先从预处理数据集开始。我们先花几个小时的时间，了解数据的分布并找出其中的规律。

Andrej有一次在整理数据时发现了重复的样本，还有一次发现了图像和标签中的错误。所以先看一眼数据能避免我们走很多弯路。

由于神经网络实际上是数据集的压缩版本，因此您将能够查看网络（错误）预测并了解它们的来源。如果你的网络给你的预测看起来与你在数据中看到的内容不一致，那么就会有所收获。

一旦从数据中发现规律，可以编写一些代码对他们进行搜索、过滤、排序。把数据可视化能帮助我们发现异常值，而异常值总能揭示数据的质量或预处理中的一些错误。

### 2、设置端到端的训练评估框架

处理完数据集，接下来就能开始训练模型了吗？并不能！下一步是建立一个完整的训练+评估框架。

在这个阶段，我们选择一个简单又不至于搞砸的模型，比如线性分类器、CNN，可视化损失。获得准确度等衡量模型的标准，用模型进行预测。

这个阶段的技巧有：

**· 固定随机种子**

使用固定的随机种子，来保证运行代码两次都获得相同的结果，消除差异因素。

**· 简单化**

在此阶段不要有任何幻想，不要扩增数据。扩增数据后面会用到，但是在这里不要使用，现在引入只会导致错误。

**· 在评估中添加有效数字**

在绘制测试集损失时，对整个测试集进行评估，不要只绘制批次测试损失图像，然后用Tensorboard对它们进行平滑处理。

**· 在初始阶段验证损失函数**

验证函数是否从正确的损失值开始。例如，如果正确初始化最后一层，则应在softmax初始化时测量-log(1/n_classes)。

**· 初始化**

正确初始化最后一层的权重。如果回归一些平均值为50的值，则将最终偏差初始化为50。如果有一个比例为1:10的不平衡数据集，请设置对数的偏差，使网络预测概率在初始化时为0.1。正确设置这些可以加速模型的收敛。

**· 人类基线**

监控除人为可解释和可检查的损失之外的指标。尽可能评估人的准确性并与之进行比较。或者对测试数据进行两次注释，并且对于每个示例，将一个注释视为预测，将第二个注释视为事实。

**· 设置一个独立于输入的基线**

最简单的方法是将所有输入设置为零，看看模型是否学会从输入中提取任何信息。

**· 过拟合一个batch**

增加了模型的容量并验证我们可以达到的最低损失。

**· 验证减少训练损失**

尝试稍微增加数据容量。

**· 在训练模型前进行数据可视化**

将原始张量的数据和标签可视化，可以节省了调试次数，并揭示了数据预处理和数据扩增中的问题。

**· 可视化预测动态**

在训练过程中对固定测试批次上的模型预测进行可视化。

**· 使用反向传播来获得依赖关系**：

一个方法是将第i个样本的损失设置为1.0，运行反向传播一直到输入，并确保仅在第i个样本上有非零的梯度。

**· 概括一个特例**：对正在做的事情编写一个非常具体的函数，让它运行，然后在以后过程中确保能得到相同的结果。

### 3、过拟合

首先我们得有一个足够大的模型，它可以过拟合，减少训练集上的损失，然后适当地调整它，放弃一些训练集损失，改善在验证集上的损失）。

这一阶段的技巧有：

**· 挑选模型**

为了获得较好的训练损失，我们需要为数据选择合适的架构。不要总想着一步到位。如果要做图像分类，只需复制粘贴ResNet-50，我们可以在稍后的过程中做一些自定义的事。

**· Adam方法是安全的**

在设定基线的早期阶段，使用学习率为3e-4的Adam 。根据经验，亚当对超参数更加宽容，包括不良的学习率。

**· 一次只复杂化一个**

如果多个信号输入分类器，建议逐个输入，然后增加复杂性，确保预期的性能逐步提升，而不要一股脑儿全放进去。比如，尝试先插入较小的图像，然后再将它们放大。

**· 不要相信学习率衰减默认值**

如果不小心，代码可能会过早地将学习率减少到零，导致模型无法收敛。我们完全禁用学习率衰减避免这种状况的发生。

### 4、正则化

理想的话，我们现在有一个大模型，在训练集上拟合好了。

现在，该正则化了。舍弃一点训练集上的准确率，可以换取验证集上的准确率。

这里有一些技巧：

**· 获取更多数据**

至今大家最偏爱的正则化方法，就是添加一些真实训练数据。

不要在一个小数据集花太大功夫，试图搞出大事情来。有精力去多收集点数据，这是唯一一个确保性能单调提升的方法。

**· 数据扩增**

把数据集做大，除了继续收集数据之外，就是扩增了。旋转，翻转，拉伸，做扩增的时候可以野性一点。

**· 有创意的扩增**

还有什么办法扩增数据集？比如域随机化 (Domain Randomization) ，模拟 (Simulation) ，巧妙的混合 (Hybrids) ，比如把数据插进场景里去。甚至可以用上GAN。

**· 预训练**

当然，就算你手握充足的数据，直接用预训练模型也没坏处。

**· 跟监督学习死磕**

不要对无监督预训练太过兴奋了。至少在视觉领域，无监督到现在也没有非常强大的成果。虽然，NLP领域有了BERT，有了会讲故事的GPT-2，但我们看到的效果很大程度上还是经过了人工挑选。

**· 输入低维一点**

把那些可能包含虚假信号的特征去掉，因为这些东西很可能造成过拟合，尤其是数据集不大的时候。

同理，如果低层细节不是那么重要的话，就输入小一点的图片，捕捉高层信息就好了。

**· 模型小一点**

许多情况下，都可以给网络加上领域知识限制 (Domain Knowledge Constraints) ，来把模型变小。

比如，以前很流行在ImageNet的骨架上放全连接层，但现在这种操作已经被平均池化取代了，大大减少了参数。

**·减小批尺寸**

对批量归一化 (Batch Normalization) 这项操作来说，小批量可能带来更好的正则化效果 (Regularization) 。

**· Dropout**

给卷积网络用dropout2d。不过使用需谨慎，因为这种操作似乎跟批量归一化不太合得来。

**· 权重衰减**

增加权重衰减 (Weight Decay) 的惩罚力度。

**· 早停法**

不用一直一直训练，可以观察验证集的损失，在快要过拟合的时候，及时喊停。

**· 也试试大点的模型**

注意，这条紧接上条 (且仅接上条) 。

我发现，大模型很容易过拟合，几乎是必然，但早停的话，模型可以表现很好。

最后的最后，如果想要更加确信，自己训练出的网络，是个不错的分类器，就把第一层的权重可视化一下，看看边缘 (Edges) 美不美。

如果第一层的过滤器看起来像噪音，就需要再搞一搞了。同理，激活 (Activations) 有时候也会看出瑕疵来，那样就要研究一下哪里出了问题。

### 5、调参

读到这里，你的AI应该已经开始探索广阔天地了。这里，有几件事需要注意。

**· 随机网格搜索**

在同时调整多个超参数的情况下，网格搜索听起来是很诱人，可以把各种设定都包含进来。

但是要记住，随机搜索才是最好的。

直觉上说，这是因为网络通常对其中一些参数比较敏感，对其他参数不那么敏感。

如果参数a是有用的，参数b起不了什么作用，就应该对a取样更彻底一些，不要只在几个固定点上多次取样。

**· 超参数优化**

世界上，有许多许多靓丽的贝叶斯超参数优化工具箱，很多小伙伴也给了这些工具好评。

但我个人的经验是，State-of-the-Art都是用实习生做出来的 (误) 。

### 6、还能怎么压榨**

当你已经找到了好用的架构和好用的超参数，还是有一些技巧，可以在帮你现有模型上获得更好的结果，榨干最后一丝潜能：

**· 模型合体**

把几个模型结合在一起，至少可以保证提升2%的准确度，不管是什么任务。

如果，你买不起太多的算力，就用蒸馏 (Distill) 把模型们集合成一个神经网络。

**· 放那让它训练吧**

通常，人类一看到损失趋于平稳，就停止训练了。

但我感觉，还是训练得昏天黑地，不知道多久了，比较好。

有一次，我意外把一个模型留在那训练了一整个寒假。

我回来的时候，它就成了State-of-the-Art。

## One More Thing

无独有偶，前两天有只“阵亡的程序猿”说：

> AWS的钱，不是花在你用了多少，而是花在你忘了关电脑。

![img](https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCQYLj62wpY5xicKlLfDCpKVjQOGt3z3sspI2GwoT5NZtuM4ZVrLjTUpiaZ52WlrXszkUMbTlVneR4Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

同学，如果你也曾经有这样的经历，那么恭喜，你也有训练出State-of-the-Art的潜力。

原文链接：

http://karpathy.github.io/2019/04/25/recipe/

— **完** —

**订阅AI内参，获取AI行业资讯**

![img](https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtAkpibldb6tu0lfWoPMdPlFKOhiaKOf4PibMlFibooQe4JdMLqxAN1PpoaQfD0RfpkkSzZsEeBzR1FLwA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**加入社群**

量子位AI社群开始招募啦，量子位社群分：AI讨论群、AI+行业群、AI技术群；



欢迎对AI感兴趣的同学，在量子位公众号（QbitAI）对话界面回复关键字“微信群”，获取入群方式。（技术群与AI+行业群需经过审核，审核较严，敬请谅解）

**诚挚招聘**

量子位正在招募编辑/记者，工作地点在北京中关村。期待有才气、有热情的同学加入我们！相关细节，请在量子位公众号(QbitAI)对话界面，回复“招聘”两个字。

![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)



**量子位** QbitAI · 头条号签约作者





վ'ᴗ' ի 追踪AI技术和产品新动态



喜欢就点「好看」吧 !



文章已于2019-04-26修改







微信扫一扫
关注该公众号



## 《深度学习》圣经"花书"经验法则中文版！

Jeff Macaluso [CVer](javascript:void(0);) *5月1日*

点击上方“**CVer**”，选择加"星标"或“置顶”

重磅干货，第一时间送达![img](https://mmbiz.qpic.cn/mmbiz_jpg/ow6przZuPIENb0m5iawutIf90N2Ub3dcPuP2KXHJvaR1Fv2FnicTuOy3KcHuIEJbd9lUyOibeXqW8tEhoJGL98qOw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

> 作者：Jeff Macaluso
>
> https://jeffmacaluso.github.io/post/DeepLearningRulesOfThumb/

![img](https://mmbiz.qpic.cn/mmbiz_png/yNnalkXE7oXgehDHTwEU5T4DR0JNCiborDVdy1oW1A1VZ8JteFdjykpFiaf1PAfyJxvOmvv0ibzXTAWdXX9QYwbpA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



当我在研究生期间，第一次学习神经网络时，我问我的教授是否有任何关于选择架构和超参数的经验法则。他的回答是：“嗯，有点，但不...” - 毕竟神经网络的选择远远多于其他机器学习算法！在阅读 Ian Goodfellow，Yoshua Bengio和Aaaron Courville的深度学习书时，我一直在思考这个问题，并决定攥写本书中列出的一系列规则。事实证明，它们中有很多可以完成许多类型的神经网络和任务。



![img](https://mmbiz.qpic.cn/mmbiz_jpg/yNnalkXE7oXgehDHTwEU5T4DR0JNCiborC6YEwpYicq9hwPwkULRAYc4x4ia4vgDsd8vHSCVsBiaqQIPnkYXL4XffQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



有趣的是，很多这些经验法则都没有得到很好的建立 - 深度学习仍然是一个相对较新的活跃研究领域，所以下面列出的很多都是研究人员最近发现的事情。



以下是我在阅读深度学习时所采用的更实用的注释。我在顶部包含了一个TL：DR来总结最重要的点，如果你没有很多时间，我建议你跳过下面第10节：实用方法论中的一些更重要的部分。



## 以下正文感谢 Cugty 同学的的翻译：

**https://zhuanlan.zhihu.com/p/61528654**



## **TL:DR**



- 尽可能使用迁移学习。否则，对于已经是广泛研究的问题，先从复制网络结构开始。

- - 网络结构应该总是由实验和验证误差来确定。
  - 更深（层多），更浅（层少）的网络更难优化，但是更容易有更好的泛化误差。

- 一定要使用 early stopping（早停），两种方法：

- - 在整个数据集上使用新的参数再次重新训练模型，在到达先前模型的早停点时停止训练。
  - 保留早停点时的参数，继续在所有数据上训练，当平均训练误差降到原先早停点以下时停止。

- 使用 dropout 或许是个好主意。

- - 使用 0.8 作为输入层的保留概率，0.5 作为隐层的保留概率。
  - dropout 会需要更大的网络和更长的迭代。

- ReLU 是理想的激活函数。它也有缺陷，因此使用 leaky ReLU 或 noisy ReLU 或许有性能提升，但是会有更多的参数要调。

- 要得到差不多可以接收的性能，至少需要每个类 5000 个数据（>=10M可以达到人类的水平或更好）

- - 如果低于 100,000个数据，使用 K 折交叉验证而不是 train/validation/test 划分的方法。

- 使用 GPU 能做到的最大的 batch size。

- - 尝试不同 batch size，从 32 开始以 2 的指数增加到 256，对于太大的模型可以从 16 开始。

- 带动量和学习率衰减的随机梯度下降是个不错的开始。

- - 动量超参数通常的值有 0.54, 0.9 和 0.99。可以随时间调整，从小值开始，然后增大。
  - 或者，使用 Adam 或 RMSProp。
  - 在分布式深度学习中使用异步 SGD。

- 学习率是最重要的超参数。如果时间有限，主要花时间来调它。

- - 可以通过绘制学习曲线（目标函数随时间的变化）来观察学习率的情况
  - 最优学习率通常高于前 100 次迭代后产生最佳性能的学习率，但不会高到出现不稳定的情况。

- 对于计算机视觉：

- - 使用数据增强，只要图像没有本质上的改变。对比度归一化是另一个安全的预处理步骤。
  - 批量归一化，池化和填充在 CNN 中经常使用。在批量归一化的情况下或许不需要 dropout。

- 对于自然语言处理：

- - LSTM 通常效果比其他网络好。
  - 预训练的词嵌入（word2vec, word2glove 等）是很强大的方法。

- 随机搜索通常比网格搜索更快的收敛到好的超参数设置。

- 调试策略：

- - 可视化模型：查看模型检测的图像样本，这对确定性能指标是否合理有帮助。
  - 可视化最差的错误情况：这可以发现预处理和打标签中存在的问题。
  - 当训练误差太高时先拟合一个小的数据集：这能检查出是欠拟合问题还是软件缺陷。
  - 监视激活和梯度的直方图：完成大概一个 epoch。这能告诉我们神经元是否饱和及其饱和频率。梯度（值？）应该是参数的 1% 左右。

## 

## **完整的笔记**



**第一部分 应用数学和机器学习基础**



**1. 介绍**



每个类别至少需要大概 5000 个样本才能得到可接受的性能。大概每类需要 10M 个样本才能达到人类水平或更好。



**4. 数值计算** 



在深度学习中，我们通常会陷入局部最优而不是全局最优，这是由于复杂性和非凸优化的问题。



**5. 机器学习基础** 



- 如果模型具有最优的容量，但是在训练和测试误差之间仍有较大差距，去收集更多的数据来。
- 通常使用 20% 的训练集作为验证集。
- 如果数据少于 100000 个样本，使用k折交叉验证而不是训练/测试集划分的方法。
- 使用均方误差时，增加容量会降低偏差但是会增加方差。
- 贝叶斯方法在训练数据有限的时候泛化性能更好，但是训练样本较多时它的计算开销很大。
- 最常使用的损失函数是负对数似然。因此最小化损失函数也就是最大似然估计。



**第二部分 深度网络：现代实践**



#### **6. 深度前馈网络**



- ReLU 对于前馈神经网络而言是一个完美的激活函数。

- - 它所基于的原则是，接近线性的特性更加容易优化。
  - sigmoid 应该用于 ReLU 不能使用的情况。例如 RNN，很多的概率模型，和一些自动编码机。

- 在基于梯度的优化问题上，由于梯度消失的原因，交叉熵相比 MSE 和 MAE 更好。

- ReLU 的优点：减少了梯度消失的概率，稀疏性，减少了计算量。

- - 缺点：有死区 Dying ReLU。（leaky 和 noisy ReLU 解决了这个问题，但是引入了额外的参数）

- 大的梯度帮助学习更快，但是任意大会导致不稳定。

- 网络结构应该通过实验和监视验证集误差来确定。

- 更深的模型减少了用于表示函数的神经元数量，也降低了泛化误差。

- - 直觉上说，更深的网络更好，因为我们在学习一系列函数。



#### **7. 深度学习的正则项**



- 最好在每层中使用不同的正则系数，但是使用相同的权重衰减。

- 使用早停 early stopping。这是一个很好调的超参数，而且也减少了计算量。

- - 在整个数据集上使用新的参数再次重新训练模型，在到达先前模型的早停点时停止训练。
  - 保留早停点时的参数，继续在所有数据上训练，当平均训练误差降到原先早停点以下时停止。

- 模型平均（bagging, boosting 等）基本上总会提高预测性能，虽然提高了计算量。

- - dropout 在宽层网络工作更好，因为它减少了从输入到输出的路径。
  - 通常输入层 dropout 中的保留概率是 0.8，隐层的概率是 0.5。
  - 使用 dropout 的模型通常要更大，迭代更长。
  - 如果数据集足够大，dropout 没有太大帮助。另外，在很小（<5000）的训练样本上dropout的作用很有限。
  - 批量标准化同样也引入了噪声，这提供了正则的作用，但也可能让 dropout 没有太大必要。
  - 模型平均一般都工作得很好，因为不同的模型不太可能犯相同的错误。
  - dropout 通过创建子网络形成了一个高效的 bagging 方法。



#### **8. 训练深度模型的优化**



- mini-batch 的大小（batch size）：大的 batch size 会提供更大的梯度，但是通常内存是个限制条件。

- - 让你的 batch size 在内存允许范围内尽可能大。
  - 在 GPU 中，从 32 到 256 以 2 的指数级别增长，对于较大的模型可以从 16 开始。
  - 小的 batch size 因为噪声的原因可能有正则的作用，但是会导致整体运行时间增加。这些情况下需要更小的学习率来提升稳定性。

- 深度学习模型有多个局部最优，但这没太大关系，因为它们都有相同的代价。最主要的问题是局部最优的损失比全局最优的损失大得多。

- - 为了测试局部最优的问题，可以绘制梯度的范数来看它是否随时间衰减到一个非常小的值。
  - 在高维非凸函数中鞍点比局部最优更常见，梯度下降对于鞍点相对来说更加鲁棒。

- 梯度裁剪（clipping）用于解决梯度爆炸的情况。这在 RNN 中是个常见的问题。

- 绘制出目标函数随时间的变化曲线来选择学习率。

- 最优学习率通常高于前 100 次迭代后产生最佳性能的学习率。监视前几次迭代，选择一个比表现最好的学习率更高的学习率，同时注意避免不稳定的情况。

- 使用高斯和均匀分布来初始看起来没有太大影响。

- - 但是，对尺度（scale）有影响。大的初始权重会帮助避免冗余神经元，但是太大会有不利影响。
  - 权重初始化可以看作是超参数，尤其是初始尺度稀疏或密集的情况。查看一个minibatch内激活或梯度的范围或方差来选择尺度。

- 没有一个优化算法明显由于其他，这主要取决于用户对超参数调整的熟悉情况。

- - 随机梯度下降（SGD），带动量的SGD，RMSProp，带动量的RMSProp，AdaDelta，Adam都是流行的选择。注意：RMSProp在训练初期或许会有很高的偏差。
  - 通常动量的值有0.5,0.9，0.99。这个超参数可以随时间变化，从一个小值开始增加到大的值。
  - Adam通常是较为鲁棒的选择。但是学习率通常要根据默认值改动。

- 对转化后的值（transformed value）而不是输入做批量归一化。在引入可学习的参数 β 下去掉偏置项。对于 CNN 在每个空间位置应用范围归一化（range normalization）。

- 太浅或太深的网络更难训练，但是他们有更好的泛化误差。

- 相对于一个强大的优化算法，选择容易优化的模型更重要。



#### **9. 卷积网络**



- 池化对于控制不同大小的输入是非常有用的。

- 0 填充可以让我们独立的设置卷积核大小和输出大小，而不会让维度衰减变成一个受限因素。

- 对于测试准确率而言最优的 0 填充通常在：

- - “valid 卷积”，不使用 0 填充，卷积核在图像范围内，但是每层输出会衰减。
  - “same 卷积”，使用足够的 0 填充让输出的大小等于输入大小。

- 一个潜在的验证卷积结构的方法是使用随机权重，并且只训练最后一层。



#### **10. 序列模型：****循环和递归网络**



- 双向 RNN 在手写识别，语音识别和生物信息方面非常成功。

- 相比较 CNN 而言，RNN 用于图像通常更困难，但是可以让相同特征图中的特征进行远程横向交互。

- 无论何时 RNN 要表示长期依赖，长期交互的梯度要指数级别小于短期交互。

- 在回波状态网络中设置权重的技术可用于在完全可训练的 RNN 中初始化权重。初始光谱半径为 1.2，稀疏初始化性能良好。

- 实践中最有效的序列模型是门控 RNN，包括 LSTM 和 GRU。

- - LSTM 比简单的 RNN 更容易学到长期依赖。
  - 给遗忘门加入偏置 1 可以让 LSTM 像 GRN 变种一样强大。
  - 在 LSTM 中使用 SGD 通常考虑二阶优化方法来防止二次偏导消失。

- 设计一个容易优化的模型通常比设计一个强大的算法容易些。

- 正则参数鼓励“信息流”，并预防梯度消失，但是同样需要梯度裁剪来预防梯度爆炸。但是大量的数据例如语言建模对于 LSTM 而言就不是那么高效了。



#### **11. 实践方法**



- 在不确定的时候让模型拒绝决策也是很有帮助的，但是这之间也存在折中。收敛是机器学习算法可以响应的样本部分，和准确率之间也存在折中。

- 对于基线模型而言，ReLU 及其变种是理想的激活函数。

- 带动量和学习率衰减的 SGD 是基线优化算法中一个不错的选择。衰减方法包括：

- - 线性衰减直到一个固定小的学习率。
  - 指数衰减。
  - 每次验证误差不变时减少 2 到 10 分之一。

- 另一个不错的基线优化算法是 Adam。

- 如果优化出现了问题，立马使用批量归一化。

- 如果训练集不足 10M，一开始就采用中等强度的正则项。

- - 基本上肯定要用 early stopping。
  - 在大多数结构中 dropout 是个不错的选择。批量归一化也是个可选的替代品。

- 如果当前的问题已经被研究烂了，直接拷贝模型就是个不错主意，或许可以拷贝训练过的模型。

- 如果已知无监督学习对于你的应用很重要（例如 NLP 中的词嵌入），那么在基线中就把它包含进来。

- 决定什么时候收集更多的数据：

- - 如果无法获得更多的数据，最后的办法是尝试提升学习算法。
  - 使用对数比例的学习曲线来决定还需要多少数据。
  - 如果训练性能很差，增加模型的大小，调整学习算法。如果还是很差，那是数据质量问题，重新开始收集更干净的数据或更多的特征。
  - 如果训练性能不错但是测试性能很差，在可行的情况下收集更多的数据。或者，尝试降低模型的大小或增加正则强度。如果这些没有帮助，那你的确需要更多的数据。

- 学习率是最重要的超参数，因为它以一种复杂的方式控制着模型的有效容量。如果时间有限，就调它。调其他的超参数需要监视训练和测试误差来判断模型是欠拟合还是过拟合。

- - 如果训练误差高于目标误差，增加容量。如果没有使用正则项，并确定优化算法工作正确，使用更多的隐层。
  - 如果测试误差高于目标误差。那么较大的模型加合适的正则会带来最好的性能。

- 只要训练误差很低，你总是可以通过收集更多数据来减低泛化误差。

- 网格搜索：通常在少于四个超参数时使用。

- - 通常最好使用对数尺度来挑选值，并且不断重复来减少搜索范围。
  - 计算量随着超参数的数量指数增加，即使是并行也不能有很好的帮助。

- 随即搜索：使用起来很简单，相比于网格搜索更快收敛到好的超参数。

- - 在几个超参数不是很强烈的影响性能指标时，随机搜索相对于网格搜索可以达到指数级别的效率。
  - 我们也许要重复运行它来得到更好的结果。
  - 随机搜索比网格搜索快，因为它不需要指数级别的运行。

- 通常不建议基于超参数来调整模型，因为它很少超过人类并且经常失败。

- 调试策略：

- - 可视化模型的行为。例如，查看图像样本，和模型检测情况。这可以看到量化的性能指标是否合理。
  - 可视化最差的错误情况：这可以发现预处理和打标签中存在的问题。
  - 当训练误差太高时先拟合一个小的数据集：这能检查出是欠拟合问题还是软件缺陷。
  - 监视激活和梯度的直方图：完成大概一个 epoch。这能告诉我们神经元是否饱和及其饱和频率。梯度值应该是参数的1%左右。稀疏的数据（如 NLP）有很多参数很少更新，一定要记得这一点。



#### **12. 应用**



- 在分布式系统中，使用异步 SGD。每一步的平均提升是很小的，但是步骤速率加快也导致了整体的加快。

- 级联（cascade）分类器是目标检测中一个高效的方法。一个分类器有高召回率，另一个有高精确率，好的，结果有了。

- 集成方法中一个减少推理时间的方法是训练一个控制器来挑选哪个网络应该来做推理。

- 标准化像素尺度是计算机视觉中唯一一个强制的预处理步骤。

- 对比度归一化通常是一个安全的计算机视觉预处理步骤。

- - 尺度参数或者可以设置为 1，或者让每个像素在整体样本上有接近1的标准梯度。
  - 近似剪切的图像数据集可以安全的设置 λ=0, ϵ=1e−8。
  - 小的随机剪切的图像数据集需要更高的正则强度，例如 λ=10, ϵ=0。
  - 全局对比度归一化（GCN）是其中一个方法，但是它在低对比度的情况下会降低边缘的检测。
  - 局部对比度归一化通常可以用分离卷积计算特征图的局部均值/方差来实现，然后在不同的特征图上做元素级别减/除。相比于全局对比度归一化更能凸显边缘。

- 在 NLP 的实践中，分层 softmax 相比于基于采样的方法测试结果更差。



**第三部分 深度学习研究**



#### **13. 线性因子模型**



- 线性因子模型可以扩展到自动编码器和深度概率模型，它们做相同的任务但是更灵活更强大。



#### **14. 自动编码器**



- 稀疏自动编码器在学习特征如分类等任务上表现不错。

- 自动编码器在学习隐含变量解释输入方面很有用。它们可以学到有用的特征。

- 虽然很多自动编码器只有一个编码/解码层，但它们有深度前馈网络一样的优点。

- - 在强化对比度例如稀疏性方面尤其有用。
  - 深度减少了指数级别的计算量，以及表示某些函数所需训练数据的数量。
  - 通常训练深度编码器的方法是通过一系列浅的自动编码器来贪心的预训练深度结构。



#### **15. 表示学习**



- 在深度学习中，一个好的表示可以让后续的学习任务更加容易。例如监督前馈网络：每一层都为最后的分类层学习一个更好的表示。

- 贪心的层间无监督训练对于分类测试误差有帮助，但是其他任务上不行。

- - 对于图像分类没啥作用，但是对于 NLP 很有帮助（例如词嵌入），这是因为初始表示非常差。
  - 在标签样本很少，或无标签样本很大的时候，正则器非常有用。
  - 在要学习的函数极其复杂的时候它最有用。
  - 根据监督阶段的验证集误差来选择预训练阶段的超参数。
  - 无监督预训练基本已经被抛弃了，除了 NLP 领域（例如词嵌入）。

- 在有些特征对于不同的任务设置有帮助的时候，迁移学习，多任务学习和域适应都可以通过表示学习来完成。

- 当一个很复杂的结构可以用更少的参数紧凑表示时，分布式表示相比于非分布式表示更具有统计优势。一些传统的非分布式算法能泛化是由于其平滑的假设，但是会受限于维度诅咒。



#### **16. 深度学习的结构化概率模型**



- 结构化概率模型提供了一个框架，用于对随机间隔的直接交互进行建模，这使得模型可以使用更少的参数。正因为此，他们可以在更少的数据下可靠地估计，并且减少了存储模型，执行推理和采样的计算量。
- 很多深度学习的生成模型或没有隐含变量，或这用一层隐含变量。他们在模型中使用深度计算图来定义条件分布。这和大部分深度学习应用中有比可观察变量更多的隐含变量形成强烈对比。他们是从非线性交互中学得的。
- 深度学习中的隐含变量是不受限的，但是很难通过可视化来解释。
- 循环信念传播（loopy belief propagation）基本上在深度学习中从未使用，因为大部分深度学习模型是使用吉布斯采样或变分推断算法设计的。



#### **17. Monte Carlo方法**



- Monte Carlo Markov Chains（MCMC）计算量很大，这是由于在平衡分布“燃烧”需要的时间，以及为了保证样本间不相关而让每n个样本有序。
- 当在深度学习中从MCMC采样时，通常需要运行一定数量的并行马尔科夫链，数量与一个minibatch的样本数一样，让后从中采样。通常使用的数字是100。
- 马尔科夫链会到达平衡状态，但我们不知道要多久，除非它已经到达了。我们可以测试它是否混合了启发式方法，比如手动检查样本或测量连续样本之间的相关性。
- 虽然 Metropolis-Hastings 算法在其他学科中经常与马尔可夫链一起使用，但 Gibbs 抽样是深度学习的 de-facto 方法。



#### **19. 近似推断**



- Maximum a posteriori (MAP) 推断通常用于特征提取和学习机制，主要是稀疏编码模型。



#### **20. 深度生成模型**



- 玻尔兹曼机的变种早已超过了最初的流行度。玻尔兹曼机对于观察变量就像一个线性预测器，但是对于未观察到的变量更加强大。

- 从一系列受限玻尔兹曼机中初始化一个深度玻尔兹曼机时，稍微修改下参数是很有必要的。

- 今天很少使用深度信念网络（DBN），因为其他算法已经超过它了，但是在历史上有重要地位。由于 DBN 是生成模型，一个训练过的 DBN 可以用于初始化一个 MLP 的权重来做分类。

- 【略去玻尔兹曼机的部分内容】

- 虽然变分自动编码器（VAE）很简单，但是它们通常能得到不错的结果，也是最后的生成模型之一。来自 VAE 的图像通常模糊，原因未知。

- 不收敛是 GAN 欠拟合（一个网络抵达局部最优，另一个抵达局部最大）的一个问题，但是这个问题的长度还不清楚。

- 虽然 GAN 有稳定性的问题，但是通常在精心选择的模型和超参数情况下效果很不错。

- GAN 的一个变种 LAPGAN，从低分辨率开始不断加入细节，它的结果经常能骗过人类。

- 为了保证 GAN 的生成器不会在任意点变成0概率，需要在最后一层给所有图像加入高斯噪声。

- 在 GAN 的判别器中一定要使用 dropout，不这样做结果很差。

- 生成矩匹配网络的视觉样本令人失望，但可以通过将它们与自动编码器结合来改进。

- 在生成图像时，使用转置卷积操作通常会产生更真实的图像，相比于没有参数共享的全连接层使用更少的参数。

- 即使在卷积生成网络中上采样的假设不真实，但是生成的样本总体来说还是不错的。

- 虽然有很多使用生成模型的方法来生成样本，MCMC 采样，ancestral sampling 或把二者结合是比较流行的做法。

- 当比较生成模型时，预处理的改变（即使很小，很微妙）是完全不能接收的，因为它会改变分布从而根本上改变了任务。

- 如果通过观察样本图片来衡量生成模型，最好在不知道样本源的情况下去做实验。另外，由于一个差的模型也可能产生好的样本，必须确保模型不是仅仅复制了训练图片。使用欧氏距离来进行检查。

- 如果计算上可行的化，最好的衡量生成模型样本的方法是评估模型分配给测试数据的对数似然。这个方法也有缺陷，例如一个固定的简单图片（如空白背景）有很高的似然。

- 生成模型有很多用处，因此根据用途来挑选评估指标。

- - 例如，一些生成模型更擅长为最真实的点分配高概率，而其他一些模型给不真实的点不分配高概率做的更好。
  - 即使当指标缩小到最合适的任务上，所有的指标也都有很严重的问题。

![img](https://mmbiz.qpic.cn/mmbiz_gif/8ib30vsyFibhrfOiaPmjayP67ejTbhm0oapUHOQOiaNV3u1ohBlkL8ficY9BJf2oZsoezRkYibbwDD7UP6wH6SwoQMOw/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

想要了解**最新/最快/最好**的论文速递、开源项目和干货资料，欢迎加入CVer学术交流知识星球。目前已有450+同学加入，涉及图像分类、目标检测、图像分割、人脸检测&识别、目标跟踪、GANs、学术竞赛交流、Re-ID、风格迁移、医学影像分析、姿态估计、OCR、SLAM、场景文字检测&识别和超分辨率等方向。



![img](https://mmbiz.qpic.cn/mmbiz_png/yNnalkXE7oVUfwCWst9bDiaMmM7fmr1dOwNaZVx5yUuILKZLEx9EmuUvMnJRxibar5bRmcWXNhvmhibPwZuXujZFA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

扫码进群



这么硬的**资料分享**，麻烦给我一个**在看**



![img](https://mmbiz.qpic.cn/mmbiz_png/e1jmIzRpwWg3jTWCAZ4BrnvIuN20lLkhIjtg4GRSDhTk9NpeF0GGTJwUpKPatscIQU7Ndj9hgl8BPpGj2BJoFw/640?tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

▲长按关注我们

**麻烦给我一个在看****！**

[阅读原文](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247488984&idx=1&sn=13a82f1d550c05ae9611508a60393aa0&chksm=f9a26757ced5ee41e6ea48913270d3e57adea3a453a4b69c96211629345f7bc1f42b0702a6de&mpshare=1&scene=24&srcid=&pass_ticket=L9MmezP0euAiloYx0ZpV0zn%2FH1NZipsFj7Qrzod8QKwih5XAiPWVLPugpr7yoxrL##)
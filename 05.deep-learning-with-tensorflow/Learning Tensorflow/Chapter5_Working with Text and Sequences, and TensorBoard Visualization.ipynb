{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Importance of Sequence Data\n",
    "As discussed in that chapter, exploiting structure is the key to success. As we will see shortly, an immensely important and useful type of structure is the sequential structure. Thinking in terms of data science, this\n",
    "fundamental structure appears in many datasets, across all domains. In computer vision, video is a sequence of visual content evolving over time. In speech we have audio signals, in genomics gene sequences; we have longitudinal medical records in healthcare, financial data in the stock market, and so on.\n",
    "\n",
    "In our MNIST data, this just means that each 28×28-pixel image can be viewed as a sequence of length 28, each element in the sequence a vector of 28 pixels. Then, the temporal dependencies in the RNN can be imaged as a scanner head, scanning the image from top to bottom (rows) or left to right (columns).\n",
    "\n",
    "### Introduction to Recurrent Neural Networks\n",
    "When we receive new information, clearly our “history” and “memory” are not wiped\n",
    "out, but instead “updated.” When we read a sentence in some text, with each new\n",
    "word, our current state of information is updated, and it is dependent not only on the\n",
    "new observed word but on the words that preceded it.\n",
    "\n",
    "A fundamental mathematical construct in statistics and probability, which is often\n",
    "used as a building block for modeling sequential patterns via machine learning is the\n",
    "Markov chain model. Figuratively speaking, we can view our data sequences as\n",
    "“chains,” with each node in the chain dependent in some way on the previous node,\n",
    "so that “history” is not erased but carried on.\n",
    "\n",
    "RNN models are also based on this notion of chain structure, and vary in how exactly\n",
    "they maintain and update information. As their name implies, recurrent neural nets\n",
    "apply some form of “loop.” As seen in Figure 5-2, at some point in time t, the network\n",
    "observes an input x t (a word in a sentence) and updates its “state vector” to h t from\n",
    "the previous vector h t-1 . When we process new input (the next word), it will be done\n",
    "in some manner that is dependent on h t and thus on the history of the sequence (the\n",
    "previous words we’ve seen affect our understanding of the current word).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vanilla RNN Implementation\n",
    "While the structure of natural images is\n",
    "well suited for CNN models, it is revealing to look at the structure of images from\n",
    "different angles. In a trend in cutting-edge deep learning research, advanced models\n",
    "attempt to exploit various kinds of sequential structures in images, trying to capture\n",
    "in some sense the “generative process” that created each image. Intuitively, this all\n",
    "comes down to the notion that nearby areas in images are somehow related, and try‐\n",
    "ing to model this structure.\n",
    "\n",
    "Here, to introduce basic RNNs and how to work with sequences, we take a simple\n",
    "sequential view of images: we look at each image in our data as a sequence of rows (or\n",
    "columns). In our MNIST data, this just means that each 28×28-pixel image can be\n",
    "viewed as a sequence of length 28, each element in the sequence a vector of 28 pixels\n",
    "(see Figure 5-3). Then, the temporal dependencies in the RNN can be imaged as a\n",
    "scanner head, scanning the image from top to bottom (rows) or left to right (col‐\n",
    "umns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-355c355d2613>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting datasets/MNIST/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting datasets/MNIST/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting datasets/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "data_dir=\"datasets/MNIST\"\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_size = 28 # D\n",
    "time_steps = 28 # T\n",
    "num_classes = 10 # \n",
    "batch_size = 128 # N\n",
    "hidden_layer_size = 128 # H\n",
    "\n",
    "# where to save tensorboard model summaries\n",
    "# TensorBoard allows you to monitor and explore the model\n",
    "# structure, weights, and training process\n",
    "log_dir = \"logs/RNN_with_summaries\"\n",
    "\n",
    "# creat placeholders for inputs and labels\n",
    "inputs = tf.placeholder(tf.float32, shape=[None,time_steps, \n",
    "                                           element_size],name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None,num_classes], name='labels')\n",
    "\n",
    "# data comes in unrolled form—a vector of 784 pixels.\n",
    "batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "# reshape data to get 28 squences of 28 pixes (N, T ,D)\n",
    "batch_x = batch_x.reshape((batch_size, time_steps,element_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We first create a function used for logging summaries, which we \n",
    "# will use later in TensorBoard\n",
    "## This helper function, taken from the official TensorFlow \n",
    "# documentation,simply adds some ops that take care of logging summaries\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        \n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            \n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        \n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wegiths and bias for input and hidden layer\n",
    "with tf.name_scope('rnn_weights'):\n",
    "    # (D,H)\n",
    "    with tf.name_scope('Wx'):\n",
    "        Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "        variable_summaries(Wx)\n",
    "    # (H,H)\n",
    "    with tf.name_scope('Wh'):\n",
    "        Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "        variable_summaries(Wh)\n",
    "    # (H,)  \n",
    "    with tf.name_scope('bias'):\n",
    "        b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        variable_summaries(b_rnn)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the RNN step with tf.scan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vanilla RNN step\n",
    "def rnn_step(previous_hidden_state, x):\n",
    "    # (N,H)\n",
    "    current_hidden_state = tf.tanh(\n",
    "        tf.matmul(x, Wx) + tf.matmul(Wh, previous_hidden_state) + b_rnn)\n",
    "    \n",
    "    return current_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply the rnn_step across all 28 time steps\n",
    "'''\n",
    "First, we reshape the inputs and then the first axis in our\n",
    "input Tensor represents the time axis, we can iterate across all time steps by using the\n",
    "built-in tf.scan() function, which repeatedly applies a callable (function) to a\n",
    "sequence of elements in order\n",
    "\n",
    "There are several advantages to this approach, chief among them the ability to have a\n",
    "dynamic number of iterations rather than fixed, computational\n",
    "speedups and optimizations for graph construction.\n",
    "'''\n",
    "# processing inputs to work with scan function \n",
    "# (batch_size, time_steps, element_size) -> (time_steps, batch_size, element_size)\n",
    "processed_inputs = tf.transpose(inputs, perm=[1,0,2])\n",
    "\n",
    "initial_hidden = tf.zeros([batch_size, hidden_layer_size])\n",
    "\n",
    "# getting all state vectors across time\n",
    "all_hidden_states = tf.scan(rnn_step, processed_inputs, \n",
    "                            initializer = initial_hidden, name=\"states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'T', b'Te', b'Ten', b'Tens', b'Tenso', b'Tensor', b'Tensor ',\n",
       "       b'Tensor F', b'Tensor Fl', b'Tensor Flo', b'Tensor Flow'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### tf.scan example ##\n",
    "elems = np.array([\"T\",\"e\",\"n\",\"s\",\"o\",\"r\", \" \", \"F\",\"l\",\"o\",\"w\"])\n",
    "scan_sum = tf.scan(lambda a, x: a + x, elems)\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(scan_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential outputs\n",
    "In an rnn step, we get a state vector for each time step, multiply it by\n",
    "some weights, and get an output vector—our new representation of the data.\n",
    "\n",
    "Our input to the RNN is sequential, and so is our output. In this sequence classifica‐\n",
    "tion example, we take the last state vector and pass it through a fully connected linear\n",
    "layer to extract an output vector (which will later be passed through a softmax activa‐\n",
    "tion function to generate predictions). This is common practice in basic sequence\n",
    "classification, where we assume that the last state vector has “accumulated” informa‐\n",
    "tion representing the entire sequence.\n",
    "\n",
    "To implement this, we first define the linear layer’s weights and bias term variables,\n",
    "and create a factory function for this layer. Then we apply this layer to all outputs\n",
    "with tf.map_fn() , which is pretty much the same as the typical map function that\n",
    "applies functions to sequences/iterables in an element-wise manner, in this case on\n",
    "each element in our sequence.\n",
    "Finally, we extract the last output for each instance in the batch, with negative index‐\n",
    "ing (similarly to ordinary Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for output layers\n",
    "with tf.name_scope(\"linear_layer_weights\") as scope:\n",
    "    # (H,C)\n",
    "    with tf.name_scope(\"W_linear\"):\n",
    "        Wl = tf.Variable(tf.truncated_normal(\n",
    "            [hidden_layer_size, num_classes], mean=0, stddev=0.01))\n",
    "        variable_summaries(Wl)\n",
    "    # (C,)    \n",
    "    with tf.name_scope(\"Bias_linar\"):\n",
    "        bl = tf.Variable(tf.truncated_normal(\n",
    "            [num_classes], mean=0, stddev=0.01))\n",
    "        variable_summaries(bl)\n",
    "    \n",
    "# apply linear layer to state vector\n",
    "def get_linear_layer(hidden_state):\n",
    "        \n",
    "    return tf.matmul(hidden_state, Wl) + bl\n",
    "\n",
    "with tf.name_scope(\"linear_layer_weights\") as scope:\n",
    "    # iterate across time, apple linear layer to all RNN outputs\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "        \n",
    "    # get last output\n",
    "    output = all_outputs[-1]\n",
    "        \n",
    "    tf.summary.histogram('outputs', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN classification\n",
    "We’re now ready to train a classifier, much in the same way we did in the previous\n",
    "chapters. We define the ops for loss function computation, optimization, and predic‐\n",
    "tion, add some more summaries for TensorBoard, and merge all these summaries\n",
    "into one operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-0dbd7280add5>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.RMSPropOptimizer(0.0005, 0.9).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y,1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0  loss: 2.302562  acc: 7.81250%\n",
      "iter: 1000  loss: 2.180588  acc: 16.40625%\n",
      "iter: 2000  loss: 1.954474  acc: 20.31250%\n",
      "iter: 3000  loss: 1.886736  acc: 29.68750%\n",
      "iter: 4000  loss: 1.681333  acc: 31.25000%\n",
      "iter: 5000  loss: 1.430858  acc: 50.00000%\n",
      "iter: 6000  loss: 1.236310  acc: 53.90625%\n",
      "iter: 7000  loss: 0.954883  acc: 67.18750%\n",
      "iter: 8000  loss: 0.906250  acc: 65.62500%\n",
      "iter: 9000  loss: 0.823342  acc: 78.90625%\n",
      "iter: 10000  loss: 0.635511  acc: 82.03125%\n",
      "iter: 11000  loss: 0.599177  acc: 80.46875%\n",
      "iter: 12000  loss: 0.535419  acc: 85.93750%\n",
      "iter: 13000  loss: 0.461119  acc: 87.50000%\n",
      "iter: 14000  loss: 0.628984  acc: 79.68750%\n",
      "iter: 15000  loss: 0.575508  acc: 80.46875%\n",
      "iter: 16000  loss: 0.554612  acc: 78.90625%\n",
      "iter: 17000  loss: 0.393890  acc: 90.62500%\n",
      "iter: 18000  loss: 0.567433  acc: 81.25000%\n",
      "iter: 19000  loss: 0.452805  acc: 90.62500%\n",
      "iter: 20000  loss: 0.591132  acc: 79.68750%\n",
      "iter: 21000  loss: 0.670028  acc: 82.81250%\n",
      "iter: 22000  loss: 0.440734  acc: 86.71875%\n",
      "iter: 23000  loss: 0.534552  acc: 79.68750%\n",
      "iter: 24000  loss: 0.599437  acc: 81.25000%\n",
      "iter: 25000  loss: 0.546581  acc: 85.15625%\n",
      "iter: 26000  loss: 0.494529  acc: 80.46875%\n",
      "iter: 27000  loss: 0.564673  acc: 82.81250%\n",
      "iter: 28000  loss: 0.609062  acc: 77.34375%\n",
      "iter: 29000  loss: 0.531439  acc: 84.37500%\n",
      "iter: 30000  loss: 0.577251  acc: 83.59375%\n",
      "iter: 31000  loss: 0.546660  acc: 83.59375%\n",
      "iter: 32000  loss: 0.466859  acc: 85.15625%\n",
      "iter: 33000  loss: 0.728129  acc: 75.78125%\n",
      "iter: 34000  loss: 0.565864  acc: 84.37500%\n",
      "iter: 35000  loss: 0.546573  acc: 80.46875%\n",
      "iter: 36000  loss: 0.463257  acc: 85.93750%\n",
      "iter: 37000  loss: 0.524874  acc: 82.81250%\n",
      "iter: 38000  loss: 0.540335  acc: 82.81250%\n",
      "iter: 39000  loss: 0.560339  acc: 83.59375%\n",
      "iter: 40000  loss: 0.574800  acc: 85.93750%\n",
      "iter: 41000  loss: 0.675804  acc: 75.00000%\n",
      "iter: 42000  loss: 0.590249  acc: 77.34375%\n",
      "iter: 43000  loss: 0.567120  acc: 81.25000%\n",
      "iter: 44000  loss: 0.540547  acc: 83.59375%\n",
      "iter: 45000  loss: 0.942689  acc: 68.75000%\n",
      "iter: 46000  loss: 0.814810  acc: 73.43750%\n",
      "iter: 47000  loss: 0.760500  acc: 71.87500%\n",
      "iter: 48000  loss: 0.640832  acc: 76.56250%\n",
      "iter: 49000  loss: 0.837326  acc: 76.56250%\n",
      "test acc: [68.75]\n"
     ]
    }
   ],
   "source": [
    "# get a small test set\n",
    "test_data = mnist.test.images[:batch_size].reshape((-1,time_steps, element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # write summaries to log_dir -- used by tensorboard\n",
    "    train_writer  = tf.summary.FileWriter(log_dir + '/train',\n",
    "                                         graph=tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(log_dir + '/test',\n",
    "                                       graph=tf.get_default_graph())\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(50000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # reshape data to get 28 squences of 28 pixels\n",
    "        batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n",
    "        \n",
    "        # train\n",
    "        summary_train, _ = sess.run([merged, train_step], \n",
    "                                    feed_dict={inputs:batch_x, y:batch_y})\n",
    "        # add to summaries\n",
    "        train_writer.add_summary(summary_train,i)\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            acc, loss = sess.run([accuracy, cross_entropy], feed_dict=\n",
    "                                 {inputs:batch_x, y:batch_y})\n",
    "            print(\"iter: {}  loss: {:.6f}  acc: {:.5f}%\".format(i, loss, acc))\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            # calculate accuracy for 128 MNIST test images and add to summaries\n",
    "            summary_test, acc = sess.run([merged, accuracy], feed_dict=\n",
    "                                   {inputs: test_data, y:test_label})\n",
    "            test_writer.add_summary(summary_test, i)\n",
    "        \n",
    "    test_acc = sess.run([accuracy], feed_dict={inputs:test_data, y:test_label})\n",
    "    print(\"test acc: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the model with TensorBoard\n",
    "TensorBoard is an interactive browser-based tool that allows us to visualize the learn‐\n",
    "ing process, as well as explore our trained model.\n",
    "To run TensorBoard, go to the command terminal and tell TensorBoard where the\n",
    "relevant summaries you logged are:\n",
    "tensorboard --logdir=LOG_DIR\n",
    "Here, LOG_DIR should be replaced with your log directory. If you are on Windows and\n",
    "this is not working, make sure you are running the terminal from the same drive\n",
    "where the log data is, and add a name to the log directory as follows in order to\n",
    "bypass a bug in the way TensorBoard parses the path:\n",
    "tensorboard --logdir=rnn_demo:LOG_DIR\n",
    "TensorBoard allows us to assign names to individual log directories by putting a\n",
    "colon between the name and the path, which may be useful when working with mul‐\n",
    "tiple log directories. In such a case, we pass a comma-separated list of log directories\n",
    "as follows:\n",
    "tensorboard --logdir=rnn_demo1:LOG_DIR1, rnn_demo2:LOG_DIR2\n",
    "In our example (with one log directory), once you have run the tensorboard com‐\n",
    "mand, you should get something like the following, telling you where to navigate in\n",
    "your browser:\n",
    "Starting TensorBoard b'39' on port 6006\n",
    "(You can navigate to http://10.100.102.4:6006)\n",
    "If the address does not work, go to localhost:6006, which should always work.\n",
    "TensorBoard recursively walks the directory tree rooted at LOG_DIR looking for sub‐\n",
    "directories that contain tfevents log data. If you run this example multiple times,\n",
    "make sure to either delete the LOG_DIR folder you created after each run, or write the\n",
    "logs to separate subdirectories within LOG_DIR , such as LOG_DIR /run1/train, LOG_DIR /\n",
    "run2/train, and so forth, to avoid issues with overwriting log files, which may lead to\n",
    "some “funky” plots.\n",
    "Let’s take a look at some of the visualizations we can get. In the next section, we will\n",
    "explore interactive visualization of high-dimensional data with TensorBoard—for\n",
    "now, we focus on plotting training process summaries and trained weights.\n",
    "First, in your browser, go to the Scalars tab. Here TensorBoard shows us summaries\n",
    "of all scalars, including not only training and testing accuracy, which are usually most\n",
    "interesting, but also some summary statistics we logged about variables (see\n",
    "Figure 5-4). Hovering over the plots, we can see some numerical figures.\n",
    "In the Graphs tab we can get an interactive visualization of our computation graph,\n",
    "from a high-level view down to the basic ops, by zooming in (see Figure 5-5).\n",
    "Finally, in the Histograms tab we see histograms of our weights across the training\n",
    "process (see Figure 5-6). Of course, we had to explicitly add these histograms to our\n",
    "logging in order to view them, with tf.summary.histogram() ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Built-in RNN Functions\n",
    "The preceding example taught us some of the fundamental and powerful ways we can\n",
    "work with sequences, by implementing our graph pretty much from scratch.\n",
    "\n",
    "tf.contrib.rnn.BasicRNNCell and tf.nn.dynamic_rnn()\n",
    "TensorFlow’s RNN cells are abstractions that represent the basic operations each\n",
    "recurrent “cell” carries out (see Figure 5-2 at the start of this chapter for an illustra‐\n",
    "tion), and its associated state. They are, in general terms, a “replacement” of the\n",
    "rnn_step() function and the associated variables it required. Of course, there are\n",
    "many variants and types of cells, each with many methods and properties. We will see\n",
    "some more advanced cells toward the end of this chapter and later in the book.\n",
    "\n",
    "Once we have created the rnn_cell , we feed it into tf.nn.dynamic_rnn() . This func‐\n",
    "tion replaces tf.scan() in our vanilla implementation and creates an RNN specified\n",
    "by rnn_cell .\n",
    "As of this writing, in early 2017, TensorFlow includes a static and a dynamic function\n",
    "for creating an RNN. What does this mean? The static version creates an unrolled\n",
    "graph (as in Figure 5-2) of fixed length. The dynamic version uses a tf.While loop to\n",
    "dynamically construct the graph at execution time, leading to faster graph creation,\n",
    "which can be significant. This dynamic construction can also be very useful in other\n",
    "ways, some of which we will touch on when we discuss variable-length sequences\n",
    "toward the end of this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-11-d3a82ccd24bd>:34: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "0 loss: 2.3093931674957275 acc: 6.2500%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desktop/tensorflow/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loss: 0.38885319232940674 acc: 88.2812%\n",
      "2000 loss: 0.17901316285133362 acc: 94.5312%\n",
      "3000 loss: 0.052633583545684814 acc: 99.2188%\n",
      "test acc: 98.4375%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('datasets/MNIST', one_hot=True)\n",
    "\n",
    "element_size = 28; time_steps =28; num_classes=10\n",
    "batch_size = 128; hidden_layer_size = 128\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None,time_steps,element_size],\n",
    "                       name='inputs')\n",
    "y = tf.placeholder(tf.float32, shape=[None, num_classes],\n",
    "                  name='inputs')\n",
    "\n",
    "# tensorflow build-in functions\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(hidden_layer_size)\n",
    "# 'cell', 'inputs', 'sequence_length=None', 'initial_state=None', 'dtype=None'\n",
    "outputs, _ = tf.nn.dynamic_rnn(rnn_cell,inputs, dtype=tf.float32)\n",
    "\n",
    "Wl = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes],\n",
    "                                    mean=0, stddev=0.01))\n",
    "bl = tf.Variable(tf.truncated_normal([num_classes],\n",
    "                                    mean=0, stddev=0.01))\n",
    "\n",
    "def get_linear_layer(vector):\n",
    "    return tf.matmul(vector,Wl) + bl\n",
    "\n",
    "last_rnn_output = outputs[:,-1,:]\n",
    "final_output = get_linear_layer(last_rnn_output)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=final_output,labels=y))\n",
    "\n",
    "train_step = tf.train.RMSPropOptimizer(0.001,0.9).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.arg_max(final_output,1), tf.arg_max(y,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "test_data = mnist.test.images[:batch_size].reshape((-1,time_steps,element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "for i in range(3001):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size,time_steps,element_size))\n",
    "    \n",
    "    sess.run(train_step, feed_dict={inputs:batch_x, y:batch_y})\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        acc, loss = sess.run([accuracy,cross_entropy], \n",
    "                             feed_dict={inputs:batch_x, y:batch_y})\n",
    "        print(\"{} loss: {} acc: {:.4f}%\".format(i, loss, acc))\n",
    "        \n",
    "\n",
    "acc_test = sess.run(accuracy, feed_dict={inputs:test_data, y:test_label})\n",
    "print(\"test acc: {:.4f}%\".format(acc_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN for Text Sequences\n",
    "In the MNIST RNN example we saw earlier, each sequence was of fixed size—the\n",
    "width (or height) of an image. Each element in the sequence was a dense vector of 28\n",
    "pixels. In NLP tasks and datasets, we have a different kind of “picture.”\n",
    "\n",
    "When creating sentences, we sample random digits and map them to the corre‐\n",
    "sponding “words” (e.g., 1 is mapped to “One,” 7 to “Seven,” etc.).\n",
    "Text sequences typically have variable lengths, which is of course the case for all real\n",
    "natural language data (such as in the sentences appearing on this page).\n",
    "\n",
    "To make our simulated sentences have different lengths, we sample for each sentence\n",
    "a random length between 3 and 6 with np.random.choice(range(3, 7)) —the lower\n",
    "bound is inclusive, and the upper bound is exclusive.\n",
    "\n",
    "Now, to put all our input sentences in one tensor (per batch of data instances), we\n",
    "need them to somehow be of the same size—so we pad sentences with a length\n",
    "shorter than 6 with zeros (or PAD symbols) to make all sentences equally sized (artifi‐\n",
    "cially). This pre-processing step is known as zero-padding. The following code\n",
    "accomplishes all of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128; embedding_dimension=64; num_classes=2\n",
    "hidden_layer_size=32; time_steps=6; element_size=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_word_map = {1:\"One\", 2:\"Two\", 3:\"Three\",\n",
    "                    4:\"Four\", 5:\"Five\", 6:\"Six\",\n",
    "                    7:\"Seven\", 8:\"Eight\", 9:\"Nine\"}\n",
    "digit_to_word_map[0] = \"PAD\"\n",
    "\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    \n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),\n",
    "                                    rand_seq_len) \n",
    "    rand_even_ints = np.random.choice(range(2,10,2),\n",
    "                                     rand_seq_len)\n",
    "    \n",
    "    ## padding\n",
    "    if rand_seq_len < 6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0]*(6-rand_seq_len))\n",
    "        \n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "        \n",
    "data = even_sentences + odd_sentences\n",
    "\n",
    "# same seq length for even , odd sentences\n",
    "# original sequence lengths\n",
    "seqlens *= 2 # 将列表内的内容复制一份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Six Six Eight Six PAD PAD', 'Eight Six Four Four PAD PAD', 'Six Six Eight PAD PAD PAD', 'Two Eight Two Six Two PAD', 'Four Six Two Eight Two PAD', 'Two Four Two Six Two PAD'] \n",
      " \n",
      " ['Three Three Nine One PAD PAD', 'Seven Nine One Nine PAD PAD', 'Five One Seven PAD PAD PAD', 'One Three Nine Nine Seven PAD', 'Nine Five Seven Seven Seven PAD', 'Three Nine Three Nine Seven PAD'] \n",
      " \n",
      " [4, 4, 3, 5, 5, 5] [4, 4, 3, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Why keep the original sentence lengths? By zero-padding, we solved one technical\n",
    "problem but created another: if we naively pass these padded sentences through our\n",
    "RNN model as they are, it will process useless PAD symbols. This would both harm\n",
    "model correctness by processing “noise” and increase computation time. We resolve\n",
    "this issue by first storing the original lengths in the seqlens array and then telling\n",
    "TensorFlow’s tf.nn.dynamic_rnn() where each sentence ends.\n",
    "'''\n",
    "\n",
    "print(even_sentences[0:6], \"\\n \\n\", odd_sentences[0:6], \n",
    "      \"\\n \\n\", seqlens[0:6], seqlens[10000:10006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "So, we now map words to indices—word identifiers—by simply creating a dictionary\n",
    "with words as keys and indices as values. We also create the inverse map. Note that\n",
    "there is no correspondence between the word IDs and the digits each word represents\n",
    "—the IDs carry no semantic meaning, just as in any NLP application with real data:\n",
    "'''\n",
    "# Map from words to indices\n",
    "word2index_map = {}\n",
    "index = 0 \n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "\n",
    "# Inverse map\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a supervised classification task—we need an array of labels in the one-hot for‐\n",
    "mat, train and test sets, a function to generate batches of instances, and placeholders,\n",
    "as usual.\n",
    "'''\n",
    "# First, we create the labels and split the data into train and test sets\n",
    "labels = [1]*10000 + [0]*10000\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1 \n",
    "    labels[i] = one_hot_encoding\n",
    "    \n",
    "## shuffle data, label and seqlen\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices] \n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate batches of sentences\n",
    "## eaach sentence in a batch is simply a list of integer IDs corresponding to teh words\n",
    "def get_sentence_batch(batch_size, data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    \n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()] \n",
    "         for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    \n",
    "    return x, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create placeholders for data\n",
    "_inputs = tf.placeholder(tf.int32, shape=[batch_size, time_steps]) # ?\n",
    "_labels = tf.placeholder(tf.int32, shape=[batch_size, num_classes])\n",
    "\n",
    "# seqlens for dynamic calculation\n",
    "_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Word Embeddings\n",
    "\n",
    "The embedding is, in a nutshell, simply a mapping from high-dimensional one-hot vec‐\n",
    "tors encoding words to lower-dimensional dense vectors. So, for example, if our\n",
    "vocabulary has size 100,000, each word in one-hot representation would be of the\n",
    "same size. The corresponding word vector—or word embedding—would be of size\n",
    "300, say. The high-dimensional one-hot vectors are thus “embedded” into a continu‐\n",
    "ous vector space with a much lower dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(tf.random_uniform(\n",
    "        [vocabulary_size, embedding_dimension], -1.0, 1.0), name='embedding')\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and Using Sequence Length\n",
    "A very popular recurrent network is the long short-term\n",
    "memory (LSTM) network. It differs from vanilla RNN by having some special mem‐\n",
    "ory mechanisms that enable the recurrent cells to better store information for long\n",
    "periods of time, thus allowing them to capture long-term dependencies better than\n",
    "plain RNN.\n",
    "\n",
    "There is nothing mysterious about these memory mechanisms; they simply consist of\n",
    "some more parameters added to each recurrent cell, enabling the RNN to overcome\n",
    "optimization issues and propagate information. These trainable parameters act as fil‐\n",
    "ters that select what information is worth “remembering” and passing on, and what is\n",
    "worth “forgetting.” They are trained in exactly the same way as any other parameter\n",
    "in a network, with gradient-descent algorithms and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-3b025a08d273>:32: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We create an LSTM cell with tf.contrib.rnn.BasicLSTMCell() and feed it to\n",
    "tf.nn.dynamic_rnn() , just as we did at the start of this chapter. We also give\n",
    "dynamic_rnn() the length of each sequence in a batch of examples, using the _seq\n",
    "lens placeholder we created earlier. TensorFlow uses this to stop all RNN steps\n",
    "beyond the last real sequence element. \n",
    "\n",
    "It also returns all output vectors over time (in\n",
    "the outputs tensor), which are all zero-padded beyond the true end of the sequence.\n",
    "So, for example, if the length of our original sequence is 5 and we zero-pad it to a\n",
    "sequence of length 15, the output for all time steps beyond 5 will be zero:\n",
    "'''\n",
    "## once the scope is used, the code can only be run once \n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\n",
    "    # states tensor -- the last valid output vector \n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n",
    "                                       sequence_length=_seqlens,\n",
    "                                       dtype=tf.float32)\n",
    "    \n",
    "weights = {'linear_layer': tf.Variable(tf.truncated_normal(\n",
    "    [hidden_layer_size, num_classes], mean=0, stddev=0.01))}\n",
    "\n",
    "biases = {'linear_layer': tf.Variable(tf.truncated_normal(\n",
    "    [num_classes], mean=0, stddev=0.01))}\n",
    "\n",
    "\n",
    "# extract the last relevant output and use in a linear layer\n",
    "final_output = tf.matmul(states[1], weights['linear_layer']) + biases['linear_layer']\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=final_output, labels=_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Embeddings and the LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc 0: 71.8750%\n",
      "training acc 100: 100.0000%\n",
      "training acc 200: 100.0000%\n",
      "training acc 300: 100.0000%\n",
      "training acc 400: 100.0000%\n",
      "training acc 500: 100.0000%\n",
      "training acc 600: 100.0000%\n",
      "training acc 700: 100.0000%\n",
      "training acc 800: 100.0000%\n",
      "training acc 900: 100.0000%\n",
      "testing acc 0: 100.0000%\n",
      "testing acc 1: 100.0000%\n",
      "testing acc 2: 100.0000%\n",
      "testing acc 3: 100.0000%\n",
      "testing acc 4: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.arg_max(final_output,1), \n",
    "                              tf.arg_max(_labels,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                                           train_x, train_y,\n",
    "                                                           train_seqlens)\n",
    "        sess.run(train_step, feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                       _seqlens:seqlen_batch})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                       _seqlens:seqlen_batch})\n",
    "            print(\"training acc {}: {:.4f}%\".format(step,acc))\n",
    "    \n",
    "    \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                        test_x, test_y,\n",
    "                                                        test_seqlens)\n",
    "        test_acc = sess.run(accuracy, feed_dict={_inputs:x_test, \n",
    "                                                 _labels:y_test,\n",
    "                                                 _seqlens:seqlen_test})\n",
    "        print(\"testing acc {}: {:.4f}%\".format(test_batch, test_acc))\n",
    "        \n",
    "    \n",
    "        output_example = sess.run(outputs, feed_dict={_inputs:x_test, \n",
    "                                                      _labels:y_test,\n",
    "                                                      _seqlens:seqlen_test})\n",
    "\n",
    "        states_example = sess.run(states, feed_dict={_inputs:x_test, \n",
    "                                                      _labels:y_test,\n",
    "                                                      _seqlens:seqlen_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlen_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 6, 32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43939957, -0.12604998, -0.4770271 ],\n",
       "       [-0.72886914, -0.5288259 , -0.860931  ],\n",
       "       [-0.83381426, -0.66460925, -0.9552554 ],\n",
       "       [-0.84083956, -0.66470736, -0.9619213 ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see that for this sentence, whose original length was 4, \n",
    "# the last two time steps have zero vectors due to padding.\n",
    "output_example[0][:6,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.84083956, -0.66470736, -0.9619213 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ????\n",
    "## We can see that it conveniently stores for us the last relevant output vector\n",
    "## —its values match the last relevant output vector before zero-padding.\n",
    "states_example[1][0][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking multiple LSTMs\n",
    "Earlier, we focused on a one-layer LSTM network for ease of exposition. Adding\n",
    "more layers is straightforward, using the MultiRNNCell() wrapper that combines\n",
    "multiple RNN cells into one multilayer cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_LSTM_layers = 2\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size,\n",
    "                                            forget_bias=1.0)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers,\n",
    "                                      state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                       sequence_length= _seqlens,\n",
    "                                       dtype=tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To get the final state of the second layer, we simply\n",
    "#  adapt our indexing a bit:\n",
    "# extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states[num_LSTM_layers-1][1], weights[\"linear_layer\"])\n",
    "               + biases[\"linear_layer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}